{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Generative Adversarial Network for gene expression inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nyo1SKr3tPOA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9cKFzBC1tPOD",
    "outputId": "247be228-a655-4b5f-c5a1-b456ab5f916e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.utils import Sequence\n",
    "from keras.optimizers import SGD\n",
    "from keras.metrics import mean_absolute_error\n",
    "from keras.layers import Input, Dense\n",
    "from keras.regularizers import l1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qsiYNDk4tPOG"
   },
   "source": [
    "# Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TTPs6AistPOG"
   },
   "source": [
    "Download the dataset from [here](https://drive.google.com/file/d/18Et3ewt471dN78tZxDHg7g-a8WI9z00H/view?usp=sharing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W_K0K4Paiyuv"
   },
   "source": [
    "### Load data to Google Colab notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "1NfsUipatXwM",
    "outputId": "3e8bad0a-99fc-4cb1-b7fa-08c25b2c158a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\r",
      "\u001b[K     |▎                               | 10kB 22.8MB/s eta 0:00:01\r",
      "\u001b[K     |▋                               | 20kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 30kB 4.8MB/s eta 0:00:01\r",
      "\u001b[K     |█▎                              | 40kB 3.1MB/s eta 0:00:01\r",
      "\u001b[K     |█▋                              | 51kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 61kB 4.5MB/s eta 0:00:01\r",
      "\u001b[K     |██▎                             | 71kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |██▋                             | 81kB 5.9MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 92kB 6.5MB/s eta 0:00:01\r",
      "\u001b[K     |███▎                            | 102kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |███▋                            | 112kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 122kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |████▎                           | 133kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |████▋                           | 143kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 153kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████▎                          | 163kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████▋                          | 174kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 184kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 194kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████▋                         | 204kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 215kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 225kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 235kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 245kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████▎                       | 256kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████▋                       | 266kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 276kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▎                      | 286kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▋                      | 296kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 307kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 317kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▋                     | 327kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 337kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▎                    | 348kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▋                    | 358kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 368kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▎                   | 378kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▋                   | 389kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 399kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▎                  | 409kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▋                  | 419kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 430kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▎                 | 440kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▋                 | 450kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 460kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▎                | 471kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▋                | 481kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 491kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▎               | 501kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▋               | 512kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 522kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▎              | 532kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▋              | 542kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 552kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 563kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 573kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 583kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 593kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▋            | 604kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 614kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▎           | 624kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▋           | 634kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 645kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 655kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 665kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 675kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 686kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▋         | 696kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 706kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▎        | 716kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 727kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 737kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 747kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▋       | 757kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 768kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 778kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 788kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▉      | 798kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 808kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 819kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 829kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▏    | 839kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 849kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 860kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▏   | 870kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▌   | 880kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 890kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 901kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▌  | 911kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▉  | 921kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▏ | 931kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▌ | 942kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▉ | 952kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 962kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 972kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 983kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 993kB 5.0MB/s \n",
      "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "title: HW6-Data.gctx, id: 18Et3ewt471dN78tZxDHg7g-a8WI9z00H\n",
      "HW6-Data.gctx\n"
     ]
    }
   ],
   "source": [
    "# Install the PyDrive wrapper & import libraries.\n",
    "# This only needs to be done once per notebook.\n",
    "!pip install -U -q PyDrive\n",
    "import os\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "# This only needs to be done once per notebook.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "\n",
    "# choose a local (colab) directory to store the data.\n",
    "local_download_path = os.path.expanduser('')\n",
    "try:\n",
    "  os.makedirs(local_download_path)\n",
    "except: pass\n",
    "\n",
    "# Download a file based on its file ID.\n",
    "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
    "file_id = '18Et3ewt471dN78tZxDHg7g-a8WI9z00H'\n",
    "downloaded = drive.CreateFile({'id': file_id})\n",
    "fname = os.path.join(local_download_path, downloaded['title'])\n",
    "# print('Downloaded content \"{}\"'.format(downloaded.GetContentString()))\n",
    "\n",
    "print('title: %s, id: %s' % (downloaded['title'], downloaded['id']))\n",
    "downloaded.GetContentFile(fname)\n",
    "print(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ySEpVV9Ai_C9"
   },
   "source": [
    "### Read .gctx data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "G6xCe-TefJze",
    "outputId": "d2c1d4c6-8346-4370-dbc4-196d94950e47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\r",
      "\u001b[K     |██▏                             | 10kB 21.8MB/s eta 0:00:01\r",
      "\u001b[K     |████▍                           | 20kB 3.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████▌                         | 30kB 5.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████▊                       | 40kB 3.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 51kB 3.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 61kB 4.6MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▎                | 71kB 5.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▌              | 81kB 6.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▋            | 92kB 6.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▉          | 102kB 5.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 112kB 5.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 122kB 5.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▍   | 133kB 5.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▌ | 143kB 5.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 153kB 5.1MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q cmapPy\n",
    "# https://github.com/cmap/cmapPy/blob/master/tutorials/cmapPy_pandasGEXpress_tutorial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9oUbD9MntPOH"
   },
   "outputs": [],
   "source": [
    "# Load and reshape data so each row corresponds to a sample\n",
    "from cmapPy.pandasGEXpress.parse import parse\n",
    "gctx_data = parse(\"HW6-Data.gctx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 692
    },
    "colab_type": "code",
    "id": "G6dclfvlgZAI",
    "outputId": "364873dd-da8f-4f12-b974-328bc0cba163"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2921, 5503)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rid</th>\n",
       "      <th>ENSG00000175063.12</th>\n",
       "      <th>ENSG00000171174.9</th>\n",
       "      <th>ENSG00000160326.9</th>\n",
       "      <th>ENSG00000204209.6</th>\n",
       "      <th>ENSG00000087460.18</th>\n",
       "      <th>ENSG00000105968.14</th>\n",
       "      <th>ENSG00000163686.9</th>\n",
       "      <th>ENSG00000079739.11</th>\n",
       "      <th>ENSG00000134057.10</th>\n",
       "      <th>ENSG00000196154.7</th>\n",
       "      <th>ENSG00000148400.9</th>\n",
       "      <th>ENSG00000164362.14</th>\n",
       "      <th>ENSG00000176171.7</th>\n",
       "      <th>ENSG00000119185.8</th>\n",
       "      <th>ENSG00000115738.5</th>\n",
       "      <th>ENSG00000171453.13</th>\n",
       "      <th>ENSG00000214063.6</th>\n",
       "      <th>ENSG00000147383.6</th>\n",
       "      <th>ENSG00000103876.7</th>\n",
       "      <th>ENSG00000109654.10</th>\n",
       "      <th>ENSG00000137843.7</th>\n",
       "      <th>ENSG00000104365.9</th>\n",
       "      <th>ENSG00000123130.12</th>\n",
       "      <th>ENSG00000100225.13</th>\n",
       "      <th>ENSG00000010810.12</th>\n",
       "      <th>ENSG00000126602.6</th>\n",
       "      <th>ENSG00000152601.13</th>\n",
       "      <th>ENSG00000149658.13</th>\n",
       "      <th>ENSG00000169598.11</th>\n",
       "      <th>ENSG00000087088.15</th>\n",
       "      <th>ENSG00000095066.7</th>\n",
       "      <th>ENSG00000161204.7</th>\n",
       "      <th>ENSG00000198369.5</th>\n",
       "      <th>ENSG00000145293.10</th>\n",
       "      <th>ENSG00000065534.14</th>\n",
       "      <th>ENSG00000072062.9</th>\n",
       "      <th>ENSG00000165704.10</th>\n",
       "      <th>ENSG00000170502.8</th>\n",
       "      <th>ENSG00000105699.12</th>\n",
       "      <th>ENSG00000172175.8</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSG00000125753.9</th>\n",
       "      <th>ENSG00000125741.4</th>\n",
       "      <th>ENSG00000064692.14</th>\n",
       "      <th>ENSG00000010310.4</th>\n",
       "      <th>ENSG00000125743.6</th>\n",
       "      <th>ENSG00000011478.7</th>\n",
       "      <th>ENSG00000177051.5</th>\n",
       "      <th>ENSG00000177045.6</th>\n",
       "      <th>ENSG00000064652.6</th>\n",
       "      <th>ENSG00000170604.3</th>\n",
       "      <th>ENSG00000151292.13</th>\n",
       "      <th>ENSG00000064199.2</th>\n",
       "      <th>ENSG00000154146.7</th>\n",
       "      <th>ENSG00000149548.10</th>\n",
       "      <th>ENSG00000182013.13</th>\n",
       "      <th>ENSG00000160013.4</th>\n",
       "      <th>ENSG00000134910.8</th>\n",
       "      <th>ENSG00000155324.5</th>\n",
       "      <th>ENSG00000090372.10</th>\n",
       "      <th>ENSG00000181027.6</th>\n",
       "      <th>ENSG00000105281.8</th>\n",
       "      <th>ENSG00000110060.4</th>\n",
       "      <th>ENSG00000113368.7</th>\n",
       "      <th>ENSG00000173926.5</th>\n",
       "      <th>ENSG00000130748.6</th>\n",
       "      <th>ENSG00000064309.10</th>\n",
       "      <th>ENSG00000130749.5</th>\n",
       "      <th>ENSG00000142230.7</th>\n",
       "      <th>ENSG00000105327.11</th>\n",
       "      <th>ENSG00000105321.8</th>\n",
       "      <th>ENSG00000064651.9</th>\n",
       "      <th>ENSG00000110063.4</th>\n",
       "      <th>ENSG00000063169.6</th>\n",
       "      <th>ENSG00000105499.9</th>\n",
       "      <th>ENSG00000142227.6</th>\n",
       "      <th>ENSG00000161558.6</th>\n",
       "      <th>ENSG00000043039.5</th>\n",
       "      <th>ENSG00000105438.4</th>\n",
       "      <th>ENSG00000105464.3</th>\n",
       "      <th>ENSG00000182324.5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GTEX-N7MS-0007-SM-2D7W1</th>\n",
       "      <td>6.258000</td>\n",
       "      <td>1.570040</td>\n",
       "      <td>12.819815</td>\n",
       "      <td>25.197998</td>\n",
       "      <td>114.751381</td>\n",
       "      <td>19.435654</td>\n",
       "      <td>1.811746</td>\n",
       "      <td>13.255496</td>\n",
       "      <td>3.265682</td>\n",
       "      <td>272.174011</td>\n",
       "      <td>18.312820</td>\n",
       "      <td>0.342227</td>\n",
       "      <td>7.193198</td>\n",
       "      <td>6.058734</td>\n",
       "      <td>35.885273</td>\n",
       "      <td>5.476271</td>\n",
       "      <td>4.680382</td>\n",
       "      <td>5.160254</td>\n",
       "      <td>5.301565</td>\n",
       "      <td>0.404902</td>\n",
       "      <td>0.447544</td>\n",
       "      <td>19.715021</td>\n",
       "      <td>12.691573</td>\n",
       "      <td>363.121033</td>\n",
       "      <td>35.115589</td>\n",
       "      <td>10.643700</td>\n",
       "      <td>18.738678</td>\n",
       "      <td>12.508470</td>\n",
       "      <td>3.961152</td>\n",
       "      <td>44.450912</td>\n",
       "      <td>5.175422</td>\n",
       "      <td>19.583858</td>\n",
       "      <td>1.780967</td>\n",
       "      <td>7.288062</td>\n",
       "      <td>1.285417</td>\n",
       "      <td>38.469940</td>\n",
       "      <td>8.974514</td>\n",
       "      <td>4.648564</td>\n",
       "      <td>8.893391</td>\n",
       "      <td>9.216136</td>\n",
       "      <td>...</td>\n",
       "      <td>127.530022</td>\n",
       "      <td>2.516836</td>\n",
       "      <td>0.008248</td>\n",
       "      <td>4.131555</td>\n",
       "      <td>20.181681</td>\n",
       "      <td>9.524962</td>\n",
       "      <td>9.139047</td>\n",
       "      <td>0.583649</td>\n",
       "      <td>1.100281</td>\n",
       "      <td>7.937450</td>\n",
       "      <td>2.745857</td>\n",
       "      <td>1.039201</td>\n",
       "      <td>60.472664</td>\n",
       "      <td>1.185207</td>\n",
       "      <td>0.084997</td>\n",
       "      <td>5.044806</td>\n",
       "      <td>14.393388</td>\n",
       "      <td>2.090897</td>\n",
       "      <td>23.079409</td>\n",
       "      <td>4.407234</td>\n",
       "      <td>19.984457</td>\n",
       "      <td>4.492527</td>\n",
       "      <td>17.262720</td>\n",
       "      <td>3.341017</td>\n",
       "      <td>5.359549</td>\n",
       "      <td>0.092620</td>\n",
       "      <td>6.270971</td>\n",
       "      <td>17.616255</td>\n",
       "      <td>13.506384</td>\n",
       "      <td>26.032234</td>\n",
       "      <td>1.507246</td>\n",
       "      <td>16.141371</td>\n",
       "      <td>8.745433</td>\n",
       "      <td>0.626008</td>\n",
       "      <td>143.632782</td>\n",
       "      <td>4.309718</td>\n",
       "      <td>0.097738</td>\n",
       "      <td>37.715435</td>\n",
       "      <td>0.264591</td>\n",
       "      <td>0.008248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GTEX-N7MS-0008-SM-4E3JI</th>\n",
       "      <td>38.256783</td>\n",
       "      <td>1.417273</td>\n",
       "      <td>3.846615</td>\n",
       "      <td>13.019768</td>\n",
       "      <td>65.975456</td>\n",
       "      <td>21.775644</td>\n",
       "      <td>2.026841</td>\n",
       "      <td>21.197948</td>\n",
       "      <td>56.756538</td>\n",
       "      <td>343.272125</td>\n",
       "      <td>2.461825</td>\n",
       "      <td>0.007284</td>\n",
       "      <td>60.472664</td>\n",
       "      <td>18.912960</td>\n",
       "      <td>29.144449</td>\n",
       "      <td>13.602143</td>\n",
       "      <td>34.434437</td>\n",
       "      <td>11.517919</td>\n",
       "      <td>6.471162</td>\n",
       "      <td>2.251328</td>\n",
       "      <td>0.007284</td>\n",
       "      <td>7.389894</td>\n",
       "      <td>12.720635</td>\n",
       "      <td>28.596855</td>\n",
       "      <td>22.336266</td>\n",
       "      <td>16.432726</td>\n",
       "      <td>28.066408</td>\n",
       "      <td>13.506384</td>\n",
       "      <td>2.540541</td>\n",
       "      <td>54.584293</td>\n",
       "      <td>3.453465</td>\n",
       "      <td>15.831875</td>\n",
       "      <td>6.571239</td>\n",
       "      <td>19.810352</td>\n",
       "      <td>3.938659</td>\n",
       "      <td>25.686693</td>\n",
       "      <td>28.365864</td>\n",
       "      <td>6.580140</td>\n",
       "      <td>1.645448</td>\n",
       "      <td>6.144692</td>\n",
       "      <td>...</td>\n",
       "      <td>38.674606</td>\n",
       "      <td>2.775685</td>\n",
       "      <td>0.404902</td>\n",
       "      <td>0.950917</td>\n",
       "      <td>90.587181</td>\n",
       "      <td>6.771498</td>\n",
       "      <td>3.824857</td>\n",
       "      <td>4.283224</td>\n",
       "      <td>4.111968</td>\n",
       "      <td>8.198557</td>\n",
       "      <td>6.310730</td>\n",
       "      <td>9.505074</td>\n",
       "      <td>5.807346</td>\n",
       "      <td>1.890565</td>\n",
       "      <td>1.013015</td>\n",
       "      <td>2.915665</td>\n",
       "      <td>47.711220</td>\n",
       "      <td>11.408774</td>\n",
       "      <td>15.766845</td>\n",
       "      <td>5.973472</td>\n",
       "      <td>67.406731</td>\n",
       "      <td>4.293163</td>\n",
       "      <td>25.409655</td>\n",
       "      <td>1.880815</td>\n",
       "      <td>15.609934</td>\n",
       "      <td>0.660897</td>\n",
       "      <td>6.270971</td>\n",
       "      <td>47.598354</td>\n",
       "      <td>2.918443</td>\n",
       "      <td>5.741512</td>\n",
       "      <td>3.179466</td>\n",
       "      <td>15.544182</td>\n",
       "      <td>3.222655</td>\n",
       "      <td>0.492695</td>\n",
       "      <td>139.820328</td>\n",
       "      <td>2.678623</td>\n",
       "      <td>0.007284</td>\n",
       "      <td>113.112526</td>\n",
       "      <td>2.248742</td>\n",
       "      <td>0.587866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GTEX-N7MS-0011-R10A-SM-2HMJK</th>\n",
       "      <td>0.018755</td>\n",
       "      <td>0.958147</td>\n",
       "      <td>9.762014</td>\n",
       "      <td>11.459002</td>\n",
       "      <td>227.624619</td>\n",
       "      <td>35.348461</td>\n",
       "      <td>8.654851</td>\n",
       "      <td>13.895129</td>\n",
       "      <td>3.640244</td>\n",
       "      <td>2.266490</td>\n",
       "      <td>2.868886</td>\n",
       "      <td>0.046608</td>\n",
       "      <td>37.521179</td>\n",
       "      <td>21.958830</td>\n",
       "      <td>46.217178</td>\n",
       "      <td>8.708959</td>\n",
       "      <td>2.130838</td>\n",
       "      <td>7.838990</td>\n",
       "      <td>1.977618</td>\n",
       "      <td>25.197998</td>\n",
       "      <td>3.091491</td>\n",
       "      <td>5.194419</td>\n",
       "      <td>3.725604</td>\n",
       "      <td>18.756191</td>\n",
       "      <td>25.409655</td>\n",
       "      <td>16.717360</td>\n",
       "      <td>9.929487</td>\n",
       "      <td>14.231618</td>\n",
       "      <td>2.824677</td>\n",
       "      <td>9.293390</td>\n",
       "      <td>7.752186</td>\n",
       "      <td>22.701349</td>\n",
       "      <td>13.101074</td>\n",
       "      <td>43.441166</td>\n",
       "      <td>3.741034</td>\n",
       "      <td>39.985050</td>\n",
       "      <td>60.657154</td>\n",
       "      <td>12.527876</td>\n",
       "      <td>3.945090</td>\n",
       "      <td>1.513837</td>\n",
       "      <td>...</td>\n",
       "      <td>4.363213</td>\n",
       "      <td>3.400148</td>\n",
       "      <td>0.699935</td>\n",
       "      <td>0.137225</td>\n",
       "      <td>49.061504</td>\n",
       "      <td>6.991272</td>\n",
       "      <td>3.069155</td>\n",
       "      <td>0.510642</td>\n",
       "      <td>5.990269</td>\n",
       "      <td>17.886543</td>\n",
       "      <td>5.495917</td>\n",
       "      <td>5.217256</td>\n",
       "      <td>977.106323</td>\n",
       "      <td>0.128216</td>\n",
       "      <td>58.799564</td>\n",
       "      <td>0.174970</td>\n",
       "      <td>6.122733</td>\n",
       "      <td>7.323215</td>\n",
       "      <td>24.217220</td>\n",
       "      <td>6.058734</td>\n",
       "      <td>0.293274</td>\n",
       "      <td>2.689233</td>\n",
       "      <td>1.145243</td>\n",
       "      <td>0.296909</td>\n",
       "      <td>38.189690</td>\n",
       "      <td>1.137451</td>\n",
       "      <td>5.081839</td>\n",
       "      <td>33.039825</td>\n",
       "      <td>2.716285</td>\n",
       "      <td>5.587399</td>\n",
       "      <td>5.591400</td>\n",
       "      <td>6.231666</td>\n",
       "      <td>3.787384</td>\n",
       "      <td>7.064671</td>\n",
       "      <td>5.623240</td>\n",
       "      <td>3.376416</td>\n",
       "      <td>0.472907</td>\n",
       "      <td>20.261278</td>\n",
       "      <td>2.430775</td>\n",
       "      <td>0.449920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GTEX-N7MS-0011-R11A-SM-2HMJS</th>\n",
       "      <td>0.579502</td>\n",
       "      <td>1.871301</td>\n",
       "      <td>4.256632</td>\n",
       "      <td>15.027001</td>\n",
       "      <td>256.823669</td>\n",
       "      <td>64.796654</td>\n",
       "      <td>10.815572</td>\n",
       "      <td>9.714371</td>\n",
       "      <td>4.323125</td>\n",
       "      <td>1.752681</td>\n",
       "      <td>0.841857</td>\n",
       "      <td>0.002584</td>\n",
       "      <td>23.311316</td>\n",
       "      <td>10.365110</td>\n",
       "      <td>5.856566</td>\n",
       "      <td>8.517663</td>\n",
       "      <td>9.796670</td>\n",
       "      <td>4.822447</td>\n",
       "      <td>2.608894</td>\n",
       "      <td>25.288555</td>\n",
       "      <td>1.106072</td>\n",
       "      <td>11.892198</td>\n",
       "      <td>3.549210</td>\n",
       "      <td>17.051687</td>\n",
       "      <td>19.198412</td>\n",
       "      <td>13.895129</td>\n",
       "      <td>7.035053</td>\n",
       "      <td>23.570780</td>\n",
       "      <td>7.614133</td>\n",
       "      <td>12.026003</td>\n",
       "      <td>9.993617</td>\n",
       "      <td>28.216610</td>\n",
       "      <td>6.457727</td>\n",
       "      <td>24.756954</td>\n",
       "      <td>0.634676</td>\n",
       "      <td>39.909748</td>\n",
       "      <td>35.764259</td>\n",
       "      <td>13.369477</td>\n",
       "      <td>1.885782</td>\n",
       "      <td>3.737905</td>\n",
       "      <td>...</td>\n",
       "      <td>3.248432</td>\n",
       "      <td>2.113203</td>\n",
       "      <td>4.253413</td>\n",
       "      <td>0.547026</td>\n",
       "      <td>42.564186</td>\n",
       "      <td>6.890395</td>\n",
       "      <td>3.251218</td>\n",
       "      <td>0.446398</td>\n",
       "      <td>3.906823</td>\n",
       "      <td>21.330336</td>\n",
       "      <td>6.364108</td>\n",
       "      <td>0.820239</td>\n",
       "      <td>1.020474</td>\n",
       "      <td>0.439290</td>\n",
       "      <td>39.243355</td>\n",
       "      <td>0.059889</td>\n",
       "      <td>8.775948</td>\n",
       "      <td>1.169072</td>\n",
       "      <td>11.150365</td>\n",
       "      <td>2.160473</td>\n",
       "      <td>0.253535</td>\n",
       "      <td>3.719441</td>\n",
       "      <td>8.482278</td>\n",
       "      <td>0.637625</td>\n",
       "      <td>11.812765</td>\n",
       "      <td>11.158457</td>\n",
       "      <td>8.733294</td>\n",
       "      <td>20.503635</td>\n",
       "      <td>7.981469</td>\n",
       "      <td>5.092885</td>\n",
       "      <td>7.415754</td>\n",
       "      <td>7.426057</td>\n",
       "      <td>7.645890</td>\n",
       "      <td>10.552812</td>\n",
       "      <td>2.984973</td>\n",
       "      <td>4.967032</td>\n",
       "      <td>0.069738</td>\n",
       "      <td>19.791620</td>\n",
       "      <td>1.013015</td>\n",
       "      <td>0.886550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GTEX-N7MS-0011-R1a-SM-2HMJG</th>\n",
       "      <td>0.065810</td>\n",
       "      <td>0.737554</td>\n",
       "      <td>11.014378</td>\n",
       "      <td>11.349816</td>\n",
       "      <td>199.399826</td>\n",
       "      <td>27.737179</td>\n",
       "      <td>21.662554</td>\n",
       "      <td>9.687140</td>\n",
       "      <td>3.774905</td>\n",
       "      <td>2.225861</td>\n",
       "      <td>4.427747</td>\n",
       "      <td>0.020815</td>\n",
       "      <td>34.379059</td>\n",
       "      <td>13.422124</td>\n",
       "      <td>14.555308</td>\n",
       "      <td>7.405584</td>\n",
       "      <td>2.529936</td>\n",
       "      <td>15.978411</td>\n",
       "      <td>3.326518</td>\n",
       "      <td>35.585335</td>\n",
       "      <td>0.728110</td>\n",
       "      <td>5.456933</td>\n",
       "      <td>2.373802</td>\n",
       "      <td>29.427275</td>\n",
       "      <td>26.656343</td>\n",
       "      <td>13.173176</td>\n",
       "      <td>12.849655</td>\n",
       "      <td>14.277672</td>\n",
       "      <td>3.549210</td>\n",
       "      <td>12.518096</td>\n",
       "      <td>5.259296</td>\n",
       "      <td>22.577900</td>\n",
       "      <td>6.771498</td>\n",
       "      <td>51.644218</td>\n",
       "      <td>4.005860</td>\n",
       "      <td>44.836121</td>\n",
       "      <td>58.799564</td>\n",
       "      <td>11.006389</td>\n",
       "      <td>4.131555</td>\n",
       "      <td>1.999842</td>\n",
       "      <td>...</td>\n",
       "      <td>4.471974</td>\n",
       "      <td>2.918443</td>\n",
       "      <td>0.515728</td>\n",
       "      <td>3.468270</td>\n",
       "      <td>55.189522</td>\n",
       "      <td>8.588790</td>\n",
       "      <td>3.309234</td>\n",
       "      <td>0.769298</td>\n",
       "      <td>6.355098</td>\n",
       "      <td>13.720300</td>\n",
       "      <td>6.575679</td>\n",
       "      <td>6.475808</td>\n",
       "      <td>184.620148</td>\n",
       "      <td>0.240404</td>\n",
       "      <td>43.265656</td>\n",
       "      <td>0.233248</td>\n",
       "      <td>6.621542</td>\n",
       "      <td>6.484859</td>\n",
       "      <td>36.313843</td>\n",
       "      <td>6.904549</td>\n",
       "      <td>0.647730</td>\n",
       "      <td>3.515989</td>\n",
       "      <td>0.510642</td>\n",
       "      <td>0.563046</td>\n",
       "      <td>36.377235</td>\n",
       "      <td>1.039201</td>\n",
       "      <td>6.399546</td>\n",
       "      <td>22.217848</td>\n",
       "      <td>1.897856</td>\n",
       "      <td>5.811523</td>\n",
       "      <td>9.418010</td>\n",
       "      <td>5.111451</td>\n",
       "      <td>4.118488</td>\n",
       "      <td>8.733294</td>\n",
       "      <td>8.905753</td>\n",
       "      <td>3.787384</td>\n",
       "      <td>0.205818</td>\n",
       "      <td>28.441481</td>\n",
       "      <td>1.129635</td>\n",
       "      <td>0.301559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5503 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "rid                           ENSG00000175063.12  ...  ENSG00000182324.5\n",
       "cid                                               ...                   \n",
       "GTEX-N7MS-0007-SM-2D7W1                 6.258000  ...           0.008248\n",
       "GTEX-N7MS-0008-SM-4E3JI                38.256783  ...           0.587866\n",
       "GTEX-N7MS-0011-R10A-SM-2HMJK            0.018755  ...           0.449920\n",
       "GTEX-N7MS-0011-R11A-SM-2HMJS            0.579502  ...           0.886550\n",
       "GTEX-N7MS-0011-R1a-SM-2HMJG             0.065810  ...           0.301559\n",
       "\n",
       "[5 rows x 5503 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = gctx_data.data_df.T\n",
    "print(data_df.shape)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xtIsrPUZjP5M"
   },
   "source": [
    "### Prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yUt3SYoitPOJ"
   },
   "outputs": [],
   "source": [
    "# Split data of landmark and target genes so each \n",
    "data = data_df.to_numpy()\n",
    "n_landmark = 943  # number of landmark genes\n",
    "n_est = np.size(data, axis=1) - n_landmark  # number of genes to be estimated\n",
    "X = data[:, :n_landmark]\n",
    "Y = data[:, n_landmark:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gecqa39rtPOK"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PX9G6px1tPON"
   },
   "outputs": [],
   "source": [
    "# Normalize data using train mean and std for each gene\n",
    "x_train_mean = x_train.mean(axis=0)\n",
    "x_train_std = x_train.std(axis=0)\n",
    "y_train_mean = y_train.mean(axis=0)\n",
    "y_train_std = y_train.std(axis=0)\n",
    "# print(\"Mean: \", train_mean)\n",
    "# print(\"Std: \", train_std)\n",
    "\n",
    "# normalization\n",
    "def normalize(x, mean, std):\n",
    "  return (x - mean) / std\n",
    "\n",
    "x_train = normalize(x_train, x_train_mean, x_train_std)\n",
    "x_test = normalize(x_test, x_train_mean, x_train_std)\n",
    "y_train = normalize(y_train, y_train_mean, y_train_std)\n",
    "y_test = normalize(y_test, y_train_mean, y_train_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DTYXNy9GtPOP"
   },
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "id": "NcOirskRtPOP",
    "outputId": "8340b8ae-6a3c-48b8-e69d-7942b524bf96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 943)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3000)              2832000   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4560)              13684560  \n",
      "=================================================================\n",
      "Total params: 16,516,560\n",
      "Trainable params: 16,516,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_predictor():\n",
    "    input_data = Input(shape=(n_landmark,))\n",
    "    h = Dense(3000, activation='linear')(input_data)\n",
    "    out = Dense(n_est, activation='linear')(h)\n",
    "    model = Model(input_data, out, name='generator')\n",
    "    return model\n",
    "\n",
    "base_model = build_predictor()\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3505
    },
    "colab_type": "code",
    "id": "hUxc0P0XtPOR",
    "outputId": "3c2ebe8f-1fbb-4b12-a8c5-0a2bc2a01a76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 2190 samples, validate on 731 samples\n",
      "Epoch 1/100\n",
      "2190/2190 [==============================] - 4s 2ms/step - loss: 0.7253 - val_loss: 0.6644\n",
      "Epoch 2/100\n",
      "2190/2190 [==============================] - 1s 473us/step - loss: 0.6192 - val_loss: 0.5848\n",
      "Epoch 3/100\n",
      "2190/2190 [==============================] - 1s 466us/step - loss: 0.5516 - val_loss: 0.5307\n",
      "Epoch 4/100\n",
      "2190/2190 [==============================] - 1s 464us/step - loss: 0.5053 - val_loss: 0.4936\n",
      "Epoch 5/100\n",
      "2190/2190 [==============================] - 1s 475us/step - loss: 0.4729 - val_loss: 0.4669\n",
      "Epoch 6/100\n",
      "2190/2190 [==============================] - 1s 470us/step - loss: 0.4490 - val_loss: 0.4467\n",
      "Epoch 7/100\n",
      "2190/2190 [==============================] - 1s 465us/step - loss: 0.4301 - val_loss: 0.4303\n",
      "Epoch 8/100\n",
      "2190/2190 [==============================] - 1s 470us/step - loss: 0.4146 - val_loss: 0.4167\n",
      "Epoch 9/100\n",
      "2190/2190 [==============================] - 1s 468us/step - loss: 0.4014 - val_loss: 0.4052\n",
      "Epoch 10/100\n",
      "2190/2190 [==============================] - 1s 468us/step - loss: 0.3901 - val_loss: 0.3953\n",
      "Epoch 11/100\n",
      "2190/2190 [==============================] - 1s 468us/step - loss: 0.3802 - val_loss: 0.3865\n",
      "Epoch 12/100\n",
      "2190/2190 [==============================] - 1s 468us/step - loss: 0.3715 - val_loss: 0.3788\n",
      "Epoch 13/100\n",
      "2190/2190 [==============================] - 1s 466us/step - loss: 0.3637 - val_loss: 0.3719\n",
      "Epoch 14/100\n",
      "2190/2190 [==============================] - 1s 470us/step - loss: 0.3566 - val_loss: 0.3656\n",
      "Epoch 15/100\n",
      "2190/2190 [==============================] - 1s 467us/step - loss: 0.3502 - val_loss: 0.3600\n",
      "Epoch 16/100\n",
      "2190/2190 [==============================] - 1s 471us/step - loss: 0.3444 - val_loss: 0.3549\n",
      "Epoch 17/100\n",
      "2190/2190 [==============================] - 1s 464us/step - loss: 0.3390 - val_loss: 0.3502\n",
      "Epoch 18/100\n",
      "2190/2190 [==============================] - 1s 463us/step - loss: 0.3341 - val_loss: 0.3458\n",
      "Epoch 19/100\n",
      "2190/2190 [==============================] - 1s 465us/step - loss: 0.3295 - val_loss: 0.3418\n",
      "Epoch 20/100\n",
      "2190/2190 [==============================] - 1s 465us/step - loss: 0.3253 - val_loss: 0.3380\n",
      "Epoch 21/100\n",
      "2190/2190 [==============================] - 1s 466us/step - loss: 0.3213 - val_loss: 0.3346\n",
      "Epoch 22/100\n",
      "2190/2190 [==============================] - 1s 467us/step - loss: 0.3176 - val_loss: 0.3313\n",
      "Epoch 23/100\n",
      "2190/2190 [==============================] - 1s 469us/step - loss: 0.3141 - val_loss: 0.3283\n",
      "Epoch 24/100\n",
      "2190/2190 [==============================] - 1s 471us/step - loss: 0.3108 - val_loss: 0.3254\n",
      "Epoch 25/100\n",
      "2190/2190 [==============================] - 1s 471us/step - loss: 0.3077 - val_loss: 0.3227\n",
      "Epoch 26/100\n",
      "2190/2190 [==============================] - 1s 471us/step - loss: 0.3047 - val_loss: 0.3202\n",
      "Epoch 27/100\n",
      "2190/2190 [==============================] - 1s 475us/step - loss: 0.3019 - val_loss: 0.3178\n",
      "Epoch 28/100\n",
      "2190/2190 [==============================] - 1s 473us/step - loss: 0.2993 - val_loss: 0.3155\n",
      "Epoch 29/100\n",
      "2190/2190 [==============================] - 1s 473us/step - loss: 0.2967 - val_loss: 0.3133\n",
      "Epoch 30/100\n",
      "2190/2190 [==============================] - 1s 478us/step - loss: 0.2943 - val_loss: 0.3112\n",
      "Epoch 31/100\n",
      "2190/2190 [==============================] - 1s 474us/step - loss: 0.2920 - val_loss: 0.3093\n",
      "Epoch 32/100\n",
      "2190/2190 [==============================] - 1s 471us/step - loss: 0.2898 - val_loss: 0.3074\n",
      "Epoch 33/100\n",
      "2190/2190 [==============================] - 1s 470us/step - loss: 0.2876 - val_loss: 0.3056\n",
      "Epoch 34/100\n",
      "2190/2190 [==============================] - 1s 470us/step - loss: 0.2856 - val_loss: 0.3038\n",
      "Epoch 35/100\n",
      "2190/2190 [==============================] - 1s 471us/step - loss: 0.2836 - val_loss: 0.3022\n",
      "Epoch 36/100\n",
      "2190/2190 [==============================] - 1s 470us/step - loss: 0.2817 - val_loss: 0.3006\n",
      "Epoch 37/100\n",
      "2190/2190 [==============================] - 1s 468us/step - loss: 0.2799 - val_loss: 0.2991\n",
      "Epoch 38/100\n",
      "2190/2190 [==============================] - 1s 465us/step - loss: 0.2781 - val_loss: 0.2976\n",
      "Epoch 39/100\n",
      "2190/2190 [==============================] - 1s 468us/step - loss: 0.2764 - val_loss: 0.2962\n",
      "Epoch 40/100\n",
      "2190/2190 [==============================] - 1s 466us/step - loss: 0.2748 - val_loss: 0.2948\n",
      "Epoch 41/100\n",
      "2190/2190 [==============================] - 1s 469us/step - loss: 0.2732 - val_loss: 0.2935\n",
      "Epoch 42/100\n",
      "2190/2190 [==============================] - 1s 467us/step - loss: 0.2716 - val_loss: 0.2922\n",
      "Epoch 43/100\n",
      "2190/2190 [==============================] - 1s 473us/step - loss: 0.2701 - val_loss: 0.2910\n",
      "Epoch 44/100\n",
      "2190/2190 [==============================] - 1s 471us/step - loss: 0.2687 - val_loss: 0.2898\n",
      "Epoch 45/100\n",
      "2190/2190 [==============================] - 1s 473us/step - loss: 0.2673 - val_loss: 0.2886\n",
      "Epoch 46/100\n",
      "2190/2190 [==============================] - 1s 476us/step - loss: 0.2659 - val_loss: 0.2874\n",
      "Epoch 47/100\n",
      "2190/2190 [==============================] - 1s 472us/step - loss: 0.2645 - val_loss: 0.2864\n",
      "Epoch 48/100\n",
      "2190/2190 [==============================] - 1s 470us/step - loss: 0.2633 - val_loss: 0.2853\n",
      "Epoch 49/100\n",
      "2190/2190 [==============================] - 1s 473us/step - loss: 0.2620 - val_loss: 0.2843\n",
      "Epoch 50/100\n",
      "2190/2190 [==============================] - 1s 471us/step - loss: 0.2608 - val_loss: 0.2833\n",
      "Epoch 51/100\n",
      "2190/2190 [==============================] - 1s 464us/step - loss: 0.2595 - val_loss: 0.2823\n",
      "Epoch 52/100\n",
      "2190/2190 [==============================] - 1s 469us/step - loss: 0.2584 - val_loss: 0.2813\n",
      "Epoch 53/100\n",
      "2190/2190 [==============================] - 1s 468us/step - loss: 0.2572 - val_loss: 0.2804\n",
      "Epoch 54/100\n",
      "2190/2190 [==============================] - 1s 465us/step - loss: 0.2561 - val_loss: 0.2795\n",
      "Epoch 55/100\n",
      "2190/2190 [==============================] - 1s 467us/step - loss: 0.2550 - val_loss: 0.2786\n",
      "Epoch 56/100\n",
      "2190/2190 [==============================] - 1s 469us/step - loss: 0.2539 - val_loss: 0.2777\n",
      "Epoch 57/100\n",
      "2190/2190 [==============================] - 1s 471us/step - loss: 0.2529 - val_loss: 0.2768\n",
      "Epoch 58/100\n",
      "2190/2190 [==============================] - 1s 466us/step - loss: 0.2519 - val_loss: 0.2761\n",
      "Epoch 59/100\n",
      "2190/2190 [==============================] - 1s 474us/step - loss: 0.2509 - val_loss: 0.2752\n",
      "Epoch 60/100\n",
      "2190/2190 [==============================] - 1s 468us/step - loss: 0.2499 - val_loss: 0.2745\n",
      "Epoch 61/100\n",
      "2190/2190 [==============================] - 1s 471us/step - loss: 0.2489 - val_loss: 0.2737\n",
      "Epoch 62/100\n",
      "2190/2190 [==============================] - 1s 463us/step - loss: 0.2480 - val_loss: 0.2730\n",
      "Epoch 63/100\n",
      "2190/2190 [==============================] - 1s 469us/step - loss: 0.2470 - val_loss: 0.2722\n",
      "Epoch 64/100\n",
      "2190/2190 [==============================] - 1s 465us/step - loss: 0.2462 - val_loss: 0.2715\n",
      "Epoch 65/100\n",
      "2190/2190 [==============================] - 1s 468us/step - loss: 0.2453 - val_loss: 0.2708\n",
      "Epoch 66/100\n",
      "2190/2190 [==============================] - 1s 468us/step - loss: 0.2444 - val_loss: 0.2701\n",
      "Epoch 67/100\n",
      "2190/2190 [==============================] - 1s 463us/step - loss: 0.2435 - val_loss: 0.2694\n",
      "Epoch 68/100\n",
      "2190/2190 [==============================] - 1s 463us/step - loss: 0.2427 - val_loss: 0.2688\n",
      "Epoch 69/100\n",
      "2190/2190 [==============================] - 1s 461us/step - loss: 0.2419 - val_loss: 0.2681\n",
      "Epoch 70/100\n",
      "2190/2190 [==============================] - 1s 464us/step - loss: 0.2411 - val_loss: 0.2674\n",
      "Epoch 71/100\n",
      "2190/2190 [==============================] - 1s 458us/step - loss: 0.2403 - val_loss: 0.2668\n",
      "Epoch 72/100\n",
      "2190/2190 [==============================] - 1s 464us/step - loss: 0.2395 - val_loss: 0.2662\n",
      "Epoch 73/100\n",
      "2190/2190 [==============================] - 1s 464us/step - loss: 0.2387 - val_loss: 0.2656\n",
      "Epoch 74/100\n",
      "2190/2190 [==============================] - 1s 469us/step - loss: 0.2380 - val_loss: 0.2650\n",
      "Epoch 75/100\n",
      "2190/2190 [==============================] - 1s 468us/step - loss: 0.2372 - val_loss: 0.2644\n",
      "Epoch 76/100\n",
      "2190/2190 [==============================] - 1s 469us/step - loss: 0.2365 - val_loss: 0.2638\n",
      "Epoch 77/100\n",
      "2190/2190 [==============================] - 1s 466us/step - loss: 0.2358 - val_loss: 0.2632\n",
      "Epoch 78/100\n",
      "2190/2190 [==============================] - 1s 468us/step - loss: 0.2351 - val_loss: 0.2627\n",
      "Epoch 79/100\n",
      "2190/2190 [==============================] - 1s 465us/step - loss: 0.2344 - val_loss: 0.2622\n",
      "Epoch 80/100\n",
      "2190/2190 [==============================] - 1s 469us/step - loss: 0.2337 - val_loss: 0.2616\n",
      "Epoch 81/100\n",
      "2190/2190 [==============================] - 1s 469us/step - loss: 0.2330 - val_loss: 0.2611\n",
      "Epoch 82/100\n",
      "2190/2190 [==============================] - 1s 472us/step - loss: 0.2324 - val_loss: 0.2606\n",
      "Epoch 83/100\n",
      "2190/2190 [==============================] - 1s 471us/step - loss: 0.2317 - val_loss: 0.2601\n",
      "Epoch 84/100\n",
      "2190/2190 [==============================] - 1s 471us/step - loss: 0.2311 - val_loss: 0.2596\n",
      "Epoch 85/100\n",
      "2190/2190 [==============================] - 1s 471us/step - loss: 0.2304 - val_loss: 0.2591\n",
      "Epoch 86/100\n",
      "2190/2190 [==============================] - 1s 477us/step - loss: 0.2298 - val_loss: 0.2586\n",
      "Epoch 87/100\n",
      "2190/2190 [==============================] - 1s 480us/step - loss: 0.2292 - val_loss: 0.2581\n",
      "Epoch 88/100\n",
      "2190/2190 [==============================] - 1s 481us/step - loss: 0.2286 - val_loss: 0.2577\n",
      "Epoch 89/100\n",
      "2190/2190 [==============================] - 1s 469us/step - loss: 0.2280 - val_loss: 0.2572\n",
      "Epoch 90/100\n",
      "2190/2190 [==============================] - 1s 465us/step - loss: 0.2274 - val_loss: 0.2568\n",
      "Epoch 91/100\n",
      "2190/2190 [==============================] - 1s 464us/step - loss: 0.2268 - val_loss: 0.2563\n",
      "Epoch 92/100\n",
      "2190/2190 [==============================] - 1s 465us/step - loss: 0.2262 - val_loss: 0.2559\n",
      "Epoch 93/100\n",
      "2190/2190 [==============================] - 1s 466us/step - loss: 0.2257 - val_loss: 0.2554\n",
      "Epoch 94/100\n",
      "2190/2190 [==============================] - 1s 464us/step - loss: 0.2251 - val_loss: 0.2550\n",
      "Epoch 95/100\n",
      "2190/2190 [==============================] - 1s 462us/step - loss: 0.2246 - val_loss: 0.2546\n",
      "Epoch 96/100\n",
      "2190/2190 [==============================] - 1s 467us/step - loss: 0.2240 - val_loss: 0.2542\n",
      "Epoch 97/100\n",
      "2190/2190 [==============================] - 1s 468us/step - loss: 0.2235 - val_loss: 0.2538\n",
      "Epoch 98/100\n",
      "2190/2190 [==============================] - 1s 466us/step - loss: 0.2230 - val_loss: 0.2534\n",
      "Epoch 99/100\n",
      "2190/2190 [==============================] - 1s 466us/step - loss: 0.2224 - val_loss: 0.2529\n",
      "Epoch 100/100\n",
      "2190/2190 [==============================] - 1s 466us/step - loss: 0.2219 - val_loss: 0.2525\n"
     ]
    }
   ],
   "source": [
    "base_model.compile(loss='mean_absolute_error', optimizer=SGD(lr=0.1))\n",
    "history = base_model.fit(x_train, y_train, \n",
    "                         epochs=100, batch_size=16, \n",
    "                         validation_data=(x_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "gmFo1N3ZtPOV",
    "outputId": "f50c1c96-267a-4721-c64c-3e2889c138c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2190/2190 [==============================] - 0s 52us/step\n",
      "Train MAE:  0.22059063235102178\n",
      "731/731 [==============================] - 0s 48us/step\n",
      "Test MAE:  0.25253316225985983\n"
     ]
    }
   ],
   "source": [
    "# report MAE on train and test data\n",
    "print(\"Train MAE: \", base_model.evaluate(x_train, y_train))\n",
    "print(\"Test MAE: \", base_model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bHo4u5P2tPOY"
   },
   "source": [
    "# Adversarial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lXYntF2COpMJ"
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "g_lr = 0.05\n",
    "d_lr = 0.1\n",
    "reg_param = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aPI6RfxMtPOZ"
   },
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    input_data = Input(shape=(n_est,))\n",
    "    hidden_layer = Dense(700, activation='linear', kernel_regularizer=l1(reg_param))(input_data) \n",
    "#                          bias_regularizer=l2(0.01))\n",
    "    output = Dense(1, activation='sigmoid')(hidden_layer)\n",
    "    model = Model(input_data, output, name='discriminator')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "Dgi36VWRtPOd",
    "outputId": "e11f73ab-b43a-45d9-e67d-3cccf6892637"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 4560)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 700)               3192700   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 701       \n",
      "=================================================================\n",
      "Total params: 3,193,401\n",
      "Trainable params: 3,193,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "discriminator = build_discriminator()\n",
    "discriminator_optimizer = SGD(lr=d_lr)\n",
    "print(discriminator.summary())\n",
    "\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=discriminator_optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "id": "YOyPg-mutPOf",
    "outputId": "4d0f159f-444a-41fb-fe24-c21d25182bcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 943)               0         \n",
      "_________________________________________________________________\n",
      "generator (Model)            (None, 4560)              16516560  \n",
      "_________________________________________________________________\n",
      "discriminator (Model)        (None, 1)                 3193401   \n",
      "=================================================================\n",
      "Total params: 19,709,961\n",
      "Trainable params: 16,516,560\n",
      "Non-trainable params: 3,193,401\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Model` call to the Keras 2 API: `Model(name=\"gan\", inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "generator = build_predictor()\n",
    "\n",
    "# Freeze discriminator weights using `trainable` parameter\n",
    "discriminator.trainable = False\n",
    "\n",
    "# then create the combined model using generator and discriminator models.\n",
    "gan_in = Input(shape=(n_landmark,), name='input')\n",
    "y_gen = generator(gan_in)\n",
    "valid = discriminator(y_gen)\n",
    "\n",
    "gan_model = Model(input=gan_in, output=[y_gen, valid], name='gan')\n",
    "print(gan_model.summary())\n",
    "# Finally compile the combined model\n",
    "gan_optimizer = SGD(lr=g_lr)\n",
    "gan_model.compile(loss={'generator': 'mean_absolute_error', 'discriminator': 'binary_crossentropy'}, \n",
    "                  optimizer=gan_optimizer, \n",
    "                  metrics={'discriminator': 'accuracy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M0TACQ6ntPOh"
   },
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    \"\"\"\n",
    "        Generates batches of data. Useful for training manually in Keras.\n",
    "        If necessary, you may update this class to fit your needs.\n",
    "        Adopted from: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "    \"\"\"\n",
    "    def __init__(self, train_X, train_y, batch_size, shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.train_X = train_X\n",
    "        self.train_y = train_y\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.train_X) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        return self.train_X[indexes], self.train_y[indexes]\n",
    "\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.train_X))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sKFvxKYitPOk"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ll9UlOOItPOm"
   },
   "outputs": [],
   "source": [
    "data_generator = DataGenerator(x_train, y_train, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "26XomG60qxj7",
    "outputId": "5dc9d76b-547a-49f2-e64f-11d53f079ea0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN metrics:  ['loss', 'generator_loss', 'discriminator_loss', 'discriminator_acc']\n",
      "discriminator metrics:  ['loss', 'acc']\n"
     ]
    }
   ],
   "source": [
    "print('GAN metrics: ', gan_model.metrics_names)\n",
    "print('discriminator metrics: ', discriminator.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 25469
    },
    "colab_type": "code",
    "id": "N00WpBxqtPOn",
    "outputId": "48a9c8d5-6630-4d4c-9d3d-82af2e0d15b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train: [D loss: 0.245199, acc.: 0.00%] [G loss: 37.272769 G MAE: 1.024674]\n",
      "0 Test: [D loss: 0.693236, acc.: 50.00%] [G loss: 16.952491 G MAE: 0.703014]\n",
      "=================================================================================\n",
      "1 Train: [D loss: 0.245162, acc.: 0.00%] [G loss: 17.270460 G MAE: 0.708476]\n",
      "1 Test: [D loss: 0.693236, acc.: 50.00%] [G loss: 16.888998 G MAE: 0.650990]\n",
      "=================================================================================\n",
      "2 Train: [D loss: 0.245132, acc.: 0.00%] [G loss: 17.188124 G MAE: 0.707990]\n",
      "2 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 16.844282 G MAE: 0.610117]\n",
      "=================================================================================\n",
      "3 Train: [D loss: 0.245114, acc.: 0.00%] [G loss: 17.127031 G MAE: 0.707596]\n",
      "3 Test: [D loss: 0.693236, acc.: 50.00%] [G loss: 16.798920 G MAE: 0.576442]\n",
      "=================================================================================\n",
      "4 Train: [D loss: 0.245110, acc.: 0.00%] [G loss: 17.061786 G MAE: 0.707177]\n",
      "4 Test: [D loss: 0.693236, acc.: 50.00%] [G loss: 16.750573 G MAE: 0.548374]\n",
      "=================================================================================\n",
      "5 Train: [D loss: 0.245115, acc.: 0.00%] [G loss: 17.004796 G MAE: 0.707184]\n",
      "5 Test: [D loss: 0.693236, acc.: 50.00%] [G loss: 16.706819 G MAE: 0.525019]\n",
      "=================================================================================\n",
      "6 Train: [D loss: 0.245108, acc.: 0.00%] [G loss: 16.947038 G MAE: 0.707197]\n",
      "6 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 16.659930 G MAE: 0.505494]\n",
      "=================================================================================\n",
      "7 Train: [D loss: 0.245120, acc.: 0.00%] [G loss: 16.891501 G MAE: 0.707112]\n",
      "7 Test: [D loss: 0.693236, acc.: 50.00%] [G loss: 16.614681 G MAE: 0.489122]\n",
      "=================================================================================\n",
      "8 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 16.842696 G MAE: 0.706964]\n",
      "8 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 16.576761 G MAE: 0.475199]\n",
      "=================================================================================\n",
      "9 Train: [D loss: 0.245119, acc.: 0.00%] [G loss: 16.799850 G MAE: 0.707015]\n",
      "9 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 16.541657 G MAE: 0.463214]\n",
      "=================================================================================\n",
      "10 Train: [D loss: 0.245109, acc.: 0.00%] [G loss: 16.761172 G MAE: 0.706984]\n",
      "10 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 16.509986 G MAE: 0.452727]\n",
      "=================================================================================\n",
      "11 Train: [D loss: 0.245102, acc.: 0.00%] [G loss: 16.723559 G MAE: 0.706899]\n",
      "11 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 16.481888 G MAE: 0.443407]\n",
      "=================================================================================\n",
      "12 Train: [D loss: 0.245108, acc.: 0.00%] [G loss: 16.693000 G MAE: 0.706909]\n",
      "12 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 16.456392 G MAE: 0.435033]\n",
      "=================================================================================\n",
      "13 Train: [D loss: 0.245109, acc.: 0.00%] [G loss: 16.663049 G MAE: 0.706991]\n",
      "13 Test: [D loss: 0.693236, acc.: 50.00%] [G loss: 16.433446 G MAE: 0.427446]\n",
      "=================================================================================\n",
      "14 Train: [D loss: 0.245104, acc.: 0.00%] [G loss: 16.635390 G MAE: 0.706950]\n",
      "14 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 16.406339 G MAE: 0.420515]\n",
      "=================================================================================\n",
      "15 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 16.607938 G MAE: 0.706900]\n",
      "15 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 16.382600 G MAE: 0.414162]\n",
      "=================================================================================\n",
      "16 Train: [D loss: 0.245105, acc.: 0.00%] [G loss: 16.578733 G MAE: 0.706928]\n",
      "16 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 16.356312 G MAE: 0.408295]\n",
      "=================================================================================\n",
      "17 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 16.552688 G MAE: 0.706871]\n",
      "17 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 16.335193 G MAE: 0.402856]\n",
      "=================================================================================\n",
      "18 Train: [D loss: 0.245103, acc.: 0.00%] [G loss: 16.529337 G MAE: 0.706859]\n",
      "18 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 16.315773 G MAE: 0.397798]\n",
      "=================================================================================\n",
      "19 Train: [D loss: 0.245106, acc.: 0.00%] [G loss: 16.506380 G MAE: 0.706859]\n",
      "19 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 16.295915 G MAE: 0.393093]\n",
      "=================================================================================\n",
      "20 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 16.484797 G MAE: 0.706884]\n",
      "20 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 16.278526 G MAE: 0.388668]\n",
      "=================================================================================\n",
      "21 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 16.465959 G MAE: 0.706842]\n",
      "21 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 16.262476 G MAE: 0.384527]\n",
      "=================================================================================\n",
      "22 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 16.448672 G MAE: 0.706876]\n",
      "22 Test: [D loss: 0.693232, acc.: 50.00%] [G loss: 16.246516 G MAE: 0.380652]\n",
      "=================================================================================\n",
      "23 Train: [D loss: 0.245104, acc.: 0.00%] [G loss: 16.432033 G MAE: 0.706854]\n",
      "23 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 16.234928 G MAE: 0.376986]\n",
      "=================================================================================\n",
      "24 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 16.415508 G MAE: 0.706848]\n",
      "24 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 16.217871 G MAE: 0.373549]\n",
      "=================================================================================\n",
      "25 Train: [D loss: 0.245103, acc.: 0.00%] [G loss: 16.398779 G MAE: 0.706877]\n",
      "25 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 16.203786 G MAE: 0.370261]\n",
      "=================================================================================\n",
      "26 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 16.381608 G MAE: 0.706839]\n",
      "26 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 16.188725 G MAE: 0.367180]\n",
      "=================================================================================\n",
      "27 Train: [D loss: 0.245103, acc.: 0.00%] [G loss: 16.366932 G MAE: 0.706761]\n",
      "27 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 16.175680 G MAE: 0.364214]\n",
      "=================================================================================\n",
      "28 Train: [D loss: 0.245103, acc.: 0.00%] [G loss: 16.351771 G MAE: 0.706898]\n",
      "28 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 16.164015 G MAE: 0.361385]\n",
      "=================================================================================\n",
      "29 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 16.338733 G MAE: 0.706792]\n",
      "29 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 16.152527 G MAE: 0.358716]\n",
      "=================================================================================\n",
      "30 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 16.325611 G MAE: 0.706836]\n",
      "30 Test: [D loss: 0.693236, acc.: 50.00%] [G loss: 16.140149 G MAE: 0.356180]\n",
      "=================================================================================\n",
      "31 Train: [D loss: 0.245112, acc.: 0.00%] [G loss: 16.309690 G MAE: 0.706865]\n",
      "31 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 16.127390 G MAE: 0.353708]\n",
      "=================================================================================\n",
      "32 Train: [D loss: 0.245108, acc.: 0.00%] [G loss: 16.296889 G MAE: 0.706754]\n",
      "32 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 16.115617 G MAE: 0.351354]\n",
      "=================================================================================\n",
      "33 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 16.282531 G MAE: 0.706744]\n",
      "33 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 16.101986 G MAE: 0.349091]\n",
      "=================================================================================\n",
      "34 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 16.270632 G MAE: 0.706768]\n",
      "34 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 16.091599 G MAE: 0.346921]\n",
      "=================================================================================\n",
      "35 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 16.258748 G MAE: 0.706820]\n",
      "35 Test: [D loss: 0.693236, acc.: 50.00%] [G loss: 16.083096 G MAE: 0.344846]\n",
      "=================================================================================\n",
      "36 Train: [D loss: 0.245106, acc.: 0.00%] [G loss: 16.249021 G MAE: 0.706778]\n",
      "36 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 16.074052 G MAE: 0.342851]\n",
      "=================================================================================\n",
      "37 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 16.238493 G MAE: 0.706775]\n",
      "37 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 16.063973 G MAE: 0.340894]\n",
      "=================================================================================\n",
      "38 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 16.227857 G MAE: 0.706868]\n",
      "38 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 16.053924 G MAE: 0.339047]\n",
      "=================================================================================\n",
      "39 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 16.216448 G MAE: 0.706769]\n",
      "39 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 16.045830 G MAE: 0.337279]\n",
      "=================================================================================\n",
      "40 Train: [D loss: 0.245102, acc.: 0.00%] [G loss: 16.207716 G MAE: 0.706717]\n",
      "40 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 16.036937 G MAE: 0.335531]\n",
      "=================================================================================\n",
      "41 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 16.197120 G MAE: 0.706827]\n",
      "41 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 16.027088 G MAE: 0.333876]\n",
      "=================================================================================\n",
      "42 Train: [D loss: 0.245102, acc.: 0.00%] [G loss: 16.187423 G MAE: 0.706753]\n",
      "42 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 16.018255 G MAE: 0.332236]\n",
      "=================================================================================\n",
      "43 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 16.177783 G MAE: 0.706741]\n",
      "43 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 16.010958 G MAE: 0.330699]\n",
      "=================================================================================\n",
      "44 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 16.168481 G MAE: 0.706714]\n",
      "44 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 16.002191 G MAE: 0.329176]\n",
      "=================================================================================\n",
      "45 Train: [D loss: 0.245094, acc.: 0.00%] [G loss: 16.158784 G MAE: 0.706771]\n",
      "45 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.991105 G MAE: 0.327704]\n",
      "=================================================================================\n",
      "46 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 16.148024 G MAE: 0.706759]\n",
      "46 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.983093 G MAE: 0.326282]\n",
      "=================================================================================\n",
      "47 Train: [D loss: 0.245102, acc.: 0.00%] [G loss: 16.138083 G MAE: 0.706745]\n",
      "47 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.973753 G MAE: 0.324914]\n",
      "=================================================================================\n",
      "48 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 16.128573 G MAE: 0.706770]\n",
      "48 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.965808 G MAE: 0.323581]\n",
      "=================================================================================\n",
      "49 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 16.120053 G MAE: 0.706764]\n",
      "49 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.958590 G MAE: 0.322269]\n",
      "=================================================================================\n",
      "50 Train: [D loss: 0.245104, acc.: 0.00%] [G loss: 16.112526 G MAE: 0.706743]\n",
      "50 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.951669 G MAE: 0.321004]\n",
      "=================================================================================\n",
      "51 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 16.104514 G MAE: 0.706766]\n",
      "51 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.944045 G MAE: 0.319760]\n",
      "=================================================================================\n",
      "52 Train: [D loss: 0.245106, acc.: 0.00%] [G loss: 16.095330 G MAE: 0.706690]\n",
      "52 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.936277 G MAE: 0.318557]\n",
      "=================================================================================\n",
      "53 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 16.086345 G MAE: 0.706718]\n",
      "53 Test: [D loss: 0.693232, acc.: 50.00%] [G loss: 15.927915 G MAE: 0.317399]\n",
      "=================================================================================\n",
      "54 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 16.078170 G MAE: 0.706700]\n",
      "54 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.920488 G MAE: 0.316258]\n",
      "=================================================================================\n",
      "55 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 16.069385 G MAE: 0.706703]\n",
      "55 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.912656 G MAE: 0.315141]\n",
      "=================================================================================\n",
      "56 Train: [D loss: 0.245092, acc.: 0.00%] [G loss: 16.062722 G MAE: 0.706701]\n",
      "56 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.906450 G MAE: 0.314065]\n",
      "=================================================================================\n",
      "57 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 16.055810 G MAE: 0.706681]\n",
      "57 Test: [D loss: 0.693232, acc.: 50.00%] [G loss: 15.900055 G MAE: 0.313020]\n",
      "=================================================================================\n",
      "58 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 16.048950 G MAE: 0.706686]\n",
      "58 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.894955 G MAE: 0.312001]\n",
      "=================================================================================\n",
      "59 Train: [D loss: 0.245102, acc.: 0.00%] [G loss: 16.043377 G MAE: 0.706740]\n",
      "59 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.889492 G MAE: 0.310983]\n",
      "=================================================================================\n",
      "60 Train: [D loss: 0.245104, acc.: 0.00%] [G loss: 16.036212 G MAE: 0.706747]\n",
      "60 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.882999 G MAE: 0.309991]\n",
      "=================================================================================\n",
      "61 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 16.028134 G MAE: 0.706699]\n",
      "61 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.875362 G MAE: 0.309063]\n",
      "=================================================================================\n",
      "62 Train: [D loss: 0.245094, acc.: 0.00%] [G loss: 16.021991 G MAE: 0.706747]\n",
      "62 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.869943 G MAE: 0.308122]\n",
      "=================================================================================\n",
      "63 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 16.016387 G MAE: 0.706684]\n",
      "63 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.865711 G MAE: 0.307200]\n",
      "=================================================================================\n",
      "64 Train: [D loss: 0.245106, acc.: 0.00%] [G loss: 16.012318 G MAE: 0.706805]\n",
      "64 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.864666 G MAE: 0.306301]\n",
      "=================================================================================\n",
      "65 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 16.009596 G MAE: 0.706724]\n",
      "65 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.860345 G MAE: 0.305432]\n",
      "=================================================================================\n",
      "66 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 16.004100 G MAE: 0.706753]\n",
      "66 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.856121 G MAE: 0.304551]\n",
      "=================================================================================\n",
      "67 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.999779 G MAE: 0.706733]\n",
      "67 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.851213 G MAE: 0.303732]\n",
      "=================================================================================\n",
      "68 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.994955 G MAE: 0.706698]\n",
      "68 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.847703 G MAE: 0.302899]\n",
      "=================================================================================\n",
      "69 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.988955 G MAE: 0.706694]\n",
      "69 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.841519 G MAE: 0.302101]\n",
      "=================================================================================\n",
      "70 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.984145 G MAE: 0.706757]\n",
      "70 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.837561 G MAE: 0.301323]\n",
      "=================================================================================\n",
      "71 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.978775 G MAE: 0.706725]\n",
      "71 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.832729 G MAE: 0.300540]\n",
      "=================================================================================\n",
      "72 Train: [D loss: 0.245104, acc.: 0.00%] [G loss: 15.973681 G MAE: 0.706665]\n",
      "72 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.827686 G MAE: 0.299767]\n",
      "=================================================================================\n",
      "73 Train: [D loss: 0.245102, acc.: 0.00%] [G loss: 15.967011 G MAE: 0.706736]\n",
      "73 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.822374 G MAE: 0.299028]\n",
      "=================================================================================\n",
      "74 Train: [D loss: 0.245094, acc.: 0.00%] [G loss: 15.961354 G MAE: 0.706666]\n",
      "74 Test: [D loss: 0.693232, acc.: 50.00%] [G loss: 15.816323 G MAE: 0.298279]\n",
      "=================================================================================\n",
      "75 Train: [D loss: 0.245106, acc.: 0.00%] [G loss: 15.955220 G MAE: 0.706689]\n",
      "75 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.810824 G MAE: 0.297567]\n",
      "=================================================================================\n",
      "76 Train: [D loss: 0.245103, acc.: 0.00%] [G loss: 15.948912 G MAE: 0.706701]\n",
      "76 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.805184 G MAE: 0.296875]\n",
      "=================================================================================\n",
      "77 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.943066 G MAE: 0.706698]\n",
      "77 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.800240 G MAE: 0.296156]\n",
      "=================================================================================\n",
      "78 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.938327 G MAE: 0.706739]\n",
      "78 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.796863 G MAE: 0.295475]\n",
      "=================================================================================\n",
      "79 Train: [D loss: 0.245104, acc.: 0.00%] [G loss: 15.935183 G MAE: 0.706770]\n",
      "79 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.792200 G MAE: 0.294807]\n",
      "=================================================================================\n",
      "80 Train: [D loss: 0.245106, acc.: 0.00%] [G loss: 15.928442 G MAE: 0.706730]\n",
      "80 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.786585 G MAE: 0.294148]\n",
      "=================================================================================\n",
      "81 Train: [D loss: 0.245092, acc.: 0.00%] [G loss: 15.922364 G MAE: 0.706695]\n",
      "81 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.781219 G MAE: 0.293489]\n",
      "=================================================================================\n",
      "82 Train: [D loss: 0.245106, acc.: 0.00%] [G loss: 15.917253 G MAE: 0.706800]\n",
      "82 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.777119 G MAE: 0.292860]\n",
      "=================================================================================\n",
      "83 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.912115 G MAE: 0.706745]\n",
      "83 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.771429 G MAE: 0.292234]\n",
      "=================================================================================\n",
      "84 Train: [D loss: 0.245104, acc.: 0.00%] [G loss: 15.906857 G MAE: 0.706720]\n",
      "84 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.767169 G MAE: 0.291615]\n",
      "=================================================================================\n",
      "85 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.901836 G MAE: 0.706710]\n",
      "85 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.762153 G MAE: 0.291006]\n",
      "=================================================================================\n",
      "86 Train: [D loss: 0.245105, acc.: 0.00%] [G loss: 15.896478 G MAE: 0.706688]\n",
      "86 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.758162 G MAE: 0.290408]\n",
      "=================================================================================\n",
      "87 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.892104 G MAE: 0.706694]\n",
      "87 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.752356 G MAE: 0.289824]\n",
      "=================================================================================\n",
      "88 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.885011 G MAE: 0.706651]\n",
      "88 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.745468 G MAE: 0.289233]\n",
      "=================================================================================\n",
      "89 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.878210 G MAE: 0.706690]\n",
      "89 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.740146 G MAE: 0.288668]\n",
      "=================================================================================\n",
      "90 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.873054 G MAE: 0.706687]\n",
      "90 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.735994 G MAE: 0.288100]\n",
      "=================================================================================\n",
      "91 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.868735 G MAE: 0.706673]\n",
      "91 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.732343 G MAE: 0.287563]\n",
      "=================================================================================\n",
      "92 Train: [D loss: 0.245093, acc.: 0.00%] [G loss: 15.866245 G MAE: 0.706686]\n",
      "92 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.730526 G MAE: 0.286995]\n",
      "=================================================================================\n",
      "93 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.862552 G MAE: 0.706698]\n",
      "93 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.727189 G MAE: 0.286467]\n",
      "=================================================================================\n",
      "94 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.857814 G MAE: 0.706679]\n",
      "94 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.720752 G MAE: 0.285919]\n",
      "=================================================================================\n",
      "95 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.852223 G MAE: 0.706705]\n",
      "95 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.717582 G MAE: 0.285406]\n",
      "=================================================================================\n",
      "96 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.848201 G MAE: 0.706660]\n",
      "96 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.713368 G MAE: 0.284900]\n",
      "=================================================================================\n",
      "97 Train: [D loss: 0.245093, acc.: 0.00%] [G loss: 15.845070 G MAE: 0.706711]\n",
      "97 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.711274 G MAE: 0.284352]\n",
      "=================================================================================\n",
      "98 Train: [D loss: 0.245114, acc.: 0.00%] [G loss: 15.842025 G MAE: 0.706657]\n",
      "98 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.708358 G MAE: 0.283861]\n",
      "=================================================================================\n",
      "99 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.837281 G MAE: 0.706707]\n",
      "99 Test: [D loss: 0.693236, acc.: 50.00%] [G loss: 15.703211 G MAE: 0.283374]\n",
      "=================================================================================\n",
      "100 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.833263 G MAE: 0.706737]\n",
      "100 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.699276 G MAE: 0.282862]\n",
      "=================================================================================\n",
      "101 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.828430 G MAE: 0.706696]\n",
      "101 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.694858 G MAE: 0.282390]\n",
      "=================================================================================\n",
      "102 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.823685 G MAE: 0.706691]\n",
      "102 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.690728 G MAE: 0.281927]\n",
      "=================================================================================\n",
      "103 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.819280 G MAE: 0.706692]\n",
      "103 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.687778 G MAE: 0.281442]\n",
      "=================================================================================\n",
      "104 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.815205 G MAE: 0.706716]\n",
      "104 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.682869 G MAE: 0.280970]\n",
      "=================================================================================\n",
      "105 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.812045 G MAE: 0.706699]\n",
      "105 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.680946 G MAE: 0.280509]\n",
      "=================================================================================\n",
      "106 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.808046 G MAE: 0.706677]\n",
      "106 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.676852 G MAE: 0.280063]\n",
      "=================================================================================\n",
      "107 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.803551 G MAE: 0.706708]\n",
      "107 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.672717 G MAE: 0.279609]\n",
      "=================================================================================\n",
      "108 Train: [D loss: 0.245103, acc.: 0.00%] [G loss: 15.800700 G MAE: 0.706734]\n",
      "108 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.670773 G MAE: 0.279175]\n",
      "=================================================================================\n",
      "109 Train: [D loss: 0.245103, acc.: 0.00%] [G loss: 15.797621 G MAE: 0.706629]\n",
      "109 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.667554 G MAE: 0.278720]\n",
      "=================================================================================\n",
      "110 Train: [D loss: 0.245104, acc.: 0.00%] [G loss: 15.794074 G MAE: 0.706713]\n",
      "110 Test: [D loss: 0.693236, acc.: 50.00%] [G loss: 15.664000 G MAE: 0.278281]\n",
      "=================================================================================\n",
      "111 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.790801 G MAE: 0.706678]\n",
      "111 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.661456 G MAE: 0.277868]\n",
      "=================================================================================\n",
      "112 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.786459 G MAE: 0.706662]\n",
      "112 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.656114 G MAE: 0.277442]\n",
      "=================================================================================\n",
      "113 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.782041 G MAE: 0.706703]\n",
      "113 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.653018 G MAE: 0.277038]\n",
      "=================================================================================\n",
      "114 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.777608 G MAE: 0.706687]\n",
      "114 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.648602 G MAE: 0.276642]\n",
      "=================================================================================\n",
      "115 Train: [D loss: 0.245103, acc.: 0.00%] [G loss: 15.773568 G MAE: 0.706651]\n",
      "115 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.645076 G MAE: 0.276227]\n",
      "=================================================================================\n",
      "116 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.770710 G MAE: 0.706685]\n",
      "116 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.643773 G MAE: 0.275829]\n",
      "=================================================================================\n",
      "117 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.767589 G MAE: 0.706690]\n",
      "117 Test: [D loss: 0.693236, acc.: 50.00%] [G loss: 15.639346 G MAE: 0.275427]\n",
      "=================================================================================\n",
      "118 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.763550 G MAE: 0.706716]\n",
      "118 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.636030 G MAE: 0.275037]\n",
      "=================================================================================\n",
      "119 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.759582 G MAE: 0.706708]\n",
      "119 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.632779 G MAE: 0.274644]\n",
      "=================================================================================\n",
      "120 Train: [D loss: 0.245110, acc.: 0.00%] [G loss: 15.756017 G MAE: 0.706651]\n",
      "120 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.629782 G MAE: 0.274255]\n",
      "=================================================================================\n",
      "121 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.753371 G MAE: 0.706639]\n",
      "121 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.627962 G MAE: 0.273875]\n",
      "=================================================================================\n",
      "122 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.751248 G MAE: 0.706675]\n",
      "122 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.625365 G MAE: 0.273472]\n",
      "=================================================================================\n",
      "123 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.747450 G MAE: 0.706648]\n",
      "123 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.621508 G MAE: 0.273115]\n",
      "=================================================================================\n",
      "124 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.743486 G MAE: 0.706673]\n",
      "124 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.618358 G MAE: 0.272724]\n",
      "=================================================================================\n",
      "125 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.740120 G MAE: 0.706701]\n",
      "125 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.615297 G MAE: 0.272361]\n",
      "=================================================================================\n",
      "126 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.736979 G MAE: 0.706677]\n",
      "126 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.612028 G MAE: 0.272017]\n",
      "=================================================================================\n",
      "127 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.733323 G MAE: 0.706709]\n",
      "127 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.608560 G MAE: 0.271642]\n",
      "=================================================================================\n",
      "128 Train: [D loss: 0.245108, acc.: 0.00%] [G loss: 15.730650 G MAE: 0.706656]\n",
      "128 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.606355 G MAE: 0.271306]\n",
      "=================================================================================\n",
      "129 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.727377 G MAE: 0.706669]\n",
      "129 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.603484 G MAE: 0.270953]\n",
      "=================================================================================\n",
      "130 Train: [D loss: 0.245103, acc.: 0.00%] [G loss: 15.724319 G MAE: 0.706661]\n",
      "130 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.600389 G MAE: 0.270603]\n",
      "=================================================================================\n",
      "131 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.720992 G MAE: 0.706717]\n",
      "131 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.597143 G MAE: 0.270246]\n",
      "=================================================================================\n",
      "132 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.717222 G MAE: 0.706653]\n",
      "132 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.594402 G MAE: 0.269919]\n",
      "=================================================================================\n",
      "133 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.714579 G MAE: 0.706693]\n",
      "133 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.591077 G MAE: 0.269564]\n",
      "=================================================================================\n",
      "134 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.711826 G MAE: 0.706722]\n",
      "134 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.588918 G MAE: 0.269251]\n",
      "=================================================================================\n",
      "135 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.709892 G MAE: 0.706725]\n",
      "135 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.587764 G MAE: 0.268910]\n",
      "=================================================================================\n",
      "136 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.707322 G MAE: 0.706679]\n",
      "136 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.584812 G MAE: 0.268592]\n",
      "=================================================================================\n",
      "137 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.703646 G MAE: 0.706668]\n",
      "137 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.581352 G MAE: 0.268270]\n",
      "=================================================================================\n",
      "138 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.701315 G MAE: 0.706653]\n",
      "138 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.579314 G MAE: 0.267928]\n",
      "=================================================================================\n",
      "139 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.698758 G MAE: 0.706727]\n",
      "139 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.576912 G MAE: 0.267616]\n",
      "=================================================================================\n",
      "140 Train: [D loss: 0.245094, acc.: 0.00%] [G loss: 15.696114 G MAE: 0.706666]\n",
      "140 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.575493 G MAE: 0.267299]\n",
      "=================================================================================\n",
      "141 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.693382 G MAE: 0.706656]\n",
      "141 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.571369 G MAE: 0.266998]\n",
      "=================================================================================\n",
      "142 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.689256 G MAE: 0.706628]\n",
      "142 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.567725 G MAE: 0.266676]\n",
      "=================================================================================\n",
      "143 Train: [D loss: 0.245103, acc.: 0.00%] [G loss: 15.685200 G MAE: 0.706611]\n",
      "143 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.563211 G MAE: 0.266382]\n",
      "=================================================================================\n",
      "144 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.681399 G MAE: 0.706658]\n",
      "144 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.561213 G MAE: 0.266074]\n",
      "=================================================================================\n",
      "145 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.678468 G MAE: 0.706625]\n",
      "145 Test: [D loss: 0.693232, acc.: 50.00%] [G loss: 15.557850 G MAE: 0.265755]\n",
      "=================================================================================\n",
      "146 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.675043 G MAE: 0.706642]\n",
      "146 Test: [D loss: 0.693232, acc.: 50.00%] [G loss: 15.555291 G MAE: 0.265465]\n",
      "=================================================================================\n",
      "147 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.672608 G MAE: 0.706665]\n",
      "147 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.552738 G MAE: 0.265177]\n",
      "=================================================================================\n",
      "148 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.669630 G MAE: 0.706661]\n",
      "148 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.549686 G MAE: 0.264901]\n",
      "=================================================================================\n",
      "149 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.665121 G MAE: 0.706691]\n",
      "149 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.545530 G MAE: 0.264613]\n",
      "=================================================================================\n",
      "150 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.662353 G MAE: 0.706645]\n",
      "150 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.543915 G MAE: 0.264297]\n",
      "=================================================================================\n",
      "151 Train: [D loss: 0.245108, acc.: 0.00%] [G loss: 15.660906 G MAE: 0.706653]\n",
      "151 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.543312 G MAE: 0.264012]\n",
      "=================================================================================\n",
      "152 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.658447 G MAE: 0.706645]\n",
      "152 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.539708 G MAE: 0.263734]\n",
      "=================================================================================\n",
      "153 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.655763 G MAE: 0.706650]\n",
      "153 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.537441 G MAE: 0.263456]\n",
      "=================================================================================\n",
      "154 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.652679 G MAE: 0.706626]\n",
      "154 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.534570 G MAE: 0.263188]\n",
      "=================================================================================\n",
      "155 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.649976 G MAE: 0.706619]\n",
      "155 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.532358 G MAE: 0.262905]\n",
      "=================================================================================\n",
      "156 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.647562 G MAE: 0.706658]\n",
      "156 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.528499 G MAE: 0.262633]\n",
      "=================================================================================\n",
      "157 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.643676 G MAE: 0.706653]\n",
      "157 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.526470 G MAE: 0.262353]\n",
      "=================================================================================\n",
      "158 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.641456 G MAE: 0.706610]\n",
      "158 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.525142 G MAE: 0.262100]\n",
      "=================================================================================\n",
      "159 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.640195 G MAE: 0.706630]\n",
      "159 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.522522 G MAE: 0.261821]\n",
      "=================================================================================\n",
      "160 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.637375 G MAE: 0.706644]\n",
      "160 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.521030 G MAE: 0.261558]\n",
      "=================================================================================\n",
      "161 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.634581 G MAE: 0.706649]\n",
      "161 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.517400 G MAE: 0.261308]\n",
      "=================================================================================\n",
      "162 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.631570 G MAE: 0.706665]\n",
      "162 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.515139 G MAE: 0.261057]\n",
      "=================================================================================\n",
      "163 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.629398 G MAE: 0.706586]\n",
      "163 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.512740 G MAE: 0.260791]\n",
      "=================================================================================\n",
      "164 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.627032 G MAE: 0.706647]\n",
      "164 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.511721 G MAE: 0.260543]\n",
      "=================================================================================\n",
      "165 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.624425 G MAE: 0.706611]\n",
      "165 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.508156 G MAE: 0.260281]\n",
      "=================================================================================\n",
      "166 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.621424 G MAE: 0.706666]\n",
      "166 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.506342 G MAE: 0.260037]\n",
      "=================================================================================\n",
      "167 Train: [D loss: 0.245103, acc.: 0.00%] [G loss: 15.619889 G MAE: 0.706703]\n",
      "167 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.505577 G MAE: 0.259776]\n",
      "=================================================================================\n",
      "168 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.617781 G MAE: 0.706674]\n",
      "168 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.502403 G MAE: 0.259525]\n",
      "=================================================================================\n",
      "169 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.614611 G MAE: 0.706651]\n",
      "169 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.499317 G MAE: 0.259285]\n",
      "=================================================================================\n",
      "170 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.611875 G MAE: 0.706654]\n",
      "170 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.496136 G MAE: 0.259043]\n",
      "=================================================================================\n",
      "171 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.607568 G MAE: 0.706643]\n",
      "171 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.492355 G MAE: 0.258801]\n",
      "=================================================================================\n",
      "172 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.603937 G MAE: 0.706663]\n",
      "172 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.488520 G MAE: 0.258558]\n",
      "=================================================================================\n",
      "173 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.599704 G MAE: 0.706671]\n",
      "173 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.484879 G MAE: 0.258335]\n",
      "=================================================================================\n",
      "174 Train: [D loss: 0.245103, acc.: 0.00%] [G loss: 15.596182 G MAE: 0.706639]\n",
      "174 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.481861 G MAE: 0.258087]\n",
      "=================================================================================\n",
      "175 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.592750 G MAE: 0.706675]\n",
      "175 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.479019 G MAE: 0.257865]\n",
      "=================================================================================\n",
      "176 Train: [D loss: 0.245107, acc.: 0.00%] [G loss: 15.590971 G MAE: 0.706661]\n",
      "176 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.478187 G MAE: 0.257635]\n",
      "=================================================================================\n",
      "177 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.589367 G MAE: 0.706667]\n",
      "177 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.476424 G MAE: 0.257400]\n",
      "=================================================================================\n",
      "178 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.587891 G MAE: 0.706714]\n",
      "178 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.474218 G MAE: 0.257174]\n",
      "=================================================================================\n",
      "179 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.585684 G MAE: 0.706632]\n",
      "179 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.472778 G MAE: 0.256939]\n",
      "=================================================================================\n",
      "180 Train: [D loss: 0.245105, acc.: 0.00%] [G loss: 15.583937 G MAE: 0.706699]\n",
      "180 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.471957 G MAE: 0.256717]\n",
      "=================================================================================\n",
      "181 Train: [D loss: 0.245108, acc.: 0.00%] [G loss: 15.582281 G MAE: 0.706677]\n",
      "181 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.469647 G MAE: 0.256492]\n",
      "=================================================================================\n",
      "182 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.580071 G MAE: 0.706691]\n",
      "182 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.467263 G MAE: 0.256272]\n",
      "=================================================================================\n",
      "183 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.576388 G MAE: 0.706643]\n",
      "183 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.464128 G MAE: 0.256039]\n",
      "=================================================================================\n",
      "184 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.573905 G MAE: 0.706732]\n",
      "184 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.461826 G MAE: 0.255837]\n",
      "=================================================================================\n",
      "185 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.571732 G MAE: 0.706627]\n",
      "185 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.460168 G MAE: 0.255611]\n",
      "=================================================================================\n",
      "186 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.569406 G MAE: 0.706613]\n",
      "186 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.457085 G MAE: 0.255397]\n",
      "=================================================================================\n",
      "187 Train: [D loss: 0.245104, acc.: 0.00%] [G loss: 15.566305 G MAE: 0.706640]\n",
      "187 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.454645 G MAE: 0.255188]\n",
      "=================================================================================\n",
      "188 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.563511 G MAE: 0.706657]\n",
      "188 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.453088 G MAE: 0.254976]\n",
      "=================================================================================\n",
      "189 Train: [D loss: 0.245093, acc.: 0.00%] [G loss: 15.562513 G MAE: 0.706647]\n",
      "189 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.450720 G MAE: 0.254774]\n",
      "=================================================================================\n",
      "190 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.559977 G MAE: 0.706649]\n",
      "190 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.449483 G MAE: 0.254557]\n",
      "=================================================================================\n",
      "191 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.558122 G MAE: 0.706661]\n",
      "191 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.447287 G MAE: 0.254345]\n",
      "=================================================================================\n",
      "192 Train: [D loss: 0.245112, acc.: 0.00%] [G loss: 15.556170 G MAE: 0.706667]\n",
      "192 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.446131 G MAE: 0.254147]\n",
      "=================================================================================\n",
      "193 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.554027 G MAE: 0.706640]\n",
      "193 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.443236 G MAE: 0.253926]\n",
      "=================================================================================\n",
      "194 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.550916 G MAE: 0.706587]\n",
      "194 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.440288 G MAE: 0.253728]\n",
      "=================================================================================\n",
      "195 Train: [D loss: 0.245093, acc.: 0.00%] [G loss: 15.548941 G MAE: 0.706630]\n",
      "195 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.439557 G MAE: 0.253549]\n",
      "=================================================================================\n",
      "196 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.548127 G MAE: 0.706695]\n",
      "196 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.437957 G MAE: 0.253350]\n",
      "=================================================================================\n",
      "197 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.545942 G MAE: 0.706643]\n",
      "197 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.435293 G MAE: 0.253143]\n",
      "=================================================================================\n",
      "198 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.543778 G MAE: 0.706693]\n",
      "198 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.434188 G MAE: 0.252940]\n",
      "=================================================================================\n",
      "199 Train: [D loss: 0.245107, acc.: 0.00%] [G loss: 15.541412 G MAE: 0.706653]\n",
      "199 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.432631 G MAE: 0.252740]\n",
      "=================================================================================\n",
      "200 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.539616 G MAE: 0.706661]\n",
      "200 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.430029 G MAE: 0.252552]\n",
      "=================================================================================\n",
      "201 Train: [D loss: 0.245104, acc.: 0.00%] [G loss: 15.537784 G MAE: 0.706656]\n",
      "201 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.429254 G MAE: 0.252362]\n",
      "=================================================================================\n",
      "202 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.535134 G MAE: 0.706619]\n",
      "202 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.425306 G MAE: 0.252158]\n",
      "=================================================================================\n",
      "203 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.532625 G MAE: 0.706671]\n",
      "203 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.423692 G MAE: 0.251966]\n",
      "=================================================================================\n",
      "204 Train: [D loss: 0.245102, acc.: 0.00%] [G loss: 15.530804 G MAE: 0.706631]\n",
      "204 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.422051 G MAE: 0.251780]\n",
      "=================================================================================\n",
      "205 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.528573 G MAE: 0.706618]\n",
      "205 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.419986 G MAE: 0.251608]\n",
      "=================================================================================\n",
      "206 Train: [D loss: 0.245103, acc.: 0.00%] [G loss: 15.526680 G MAE: 0.706676]\n",
      "206 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.418155 G MAE: 0.251419]\n",
      "=================================================================================\n",
      "207 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.525379 G MAE: 0.706705]\n",
      "207 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.416760 G MAE: 0.251226]\n",
      "=================================================================================\n",
      "208 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.522597 G MAE: 0.706628]\n",
      "208 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.414806 G MAE: 0.251036]\n",
      "=================================================================================\n",
      "209 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.521212 G MAE: 0.706616]\n",
      "209 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.413640 G MAE: 0.250860]\n",
      "=================================================================================\n",
      "210 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.519620 G MAE: 0.706646]\n",
      "210 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.411109 G MAE: 0.250675]\n",
      "=================================================================================\n",
      "211 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.517222 G MAE: 0.706648]\n",
      "211 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.409742 G MAE: 0.250495]\n",
      "=================================================================================\n",
      "212 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.515183 G MAE: 0.706666]\n",
      "212 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.406630 G MAE: 0.250324]\n",
      "=================================================================================\n",
      "213 Train: [D loss: 0.245106, acc.: 0.00%] [G loss: 15.512498 G MAE: 0.706660]\n",
      "213 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.405851 G MAE: 0.250147]\n",
      "=================================================================================\n",
      "214 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.510914 G MAE: 0.706647]\n",
      "214 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.404025 G MAE: 0.249959]\n",
      "=================================================================================\n",
      "215 Train: [D loss: 0.245104, acc.: 0.00%] [G loss: 15.509414 G MAE: 0.706672]\n",
      "215 Test: [D loss: 0.693236, acc.: 50.00%] [G loss: 15.402456 G MAE: 0.249774]\n",
      "=================================================================================\n",
      "216 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.507476 G MAE: 0.706684]\n",
      "216 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.400442 G MAE: 0.249596]\n",
      "=================================================================================\n",
      "217 Train: [D loss: 0.245094, acc.: 0.00%] [G loss: 15.505450 G MAE: 0.706632]\n",
      "217 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.398496 G MAE: 0.249453]\n",
      "=================================================================================\n",
      "218 Train: [D loss: 0.245103, acc.: 0.00%] [G loss: 15.503353 G MAE: 0.706638]\n",
      "218 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.396426 G MAE: 0.249275]\n",
      "=================================================================================\n",
      "219 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.499952 G MAE: 0.706639]\n",
      "219 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.392813 G MAE: 0.249098]\n",
      "=================================================================================\n",
      "220 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.497550 G MAE: 0.706618]\n",
      "220 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.390550 G MAE: 0.248936]\n",
      "=================================================================================\n",
      "221 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.495562 G MAE: 0.706709]\n",
      "221 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.389395 G MAE: 0.248764]\n",
      "=================================================================================\n",
      "222 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.494444 G MAE: 0.706638]\n",
      "222 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.388728 G MAE: 0.248586]\n",
      "=================================================================================\n",
      "223 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.492784 G MAE: 0.706704]\n",
      "223 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.386623 G MAE: 0.248427]\n",
      "=================================================================================\n",
      "224 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.490966 G MAE: 0.706687]\n",
      "224 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.385118 G MAE: 0.248271]\n",
      "=================================================================================\n",
      "225 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.488895 G MAE: 0.706665]\n",
      "225 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.382469 G MAE: 0.248093]\n",
      "=================================================================================\n",
      "226 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.486474 G MAE: 0.706640]\n",
      "226 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.381239 G MAE: 0.247936]\n",
      "=================================================================================\n",
      "227 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.483878 G MAE: 0.706645]\n",
      "227 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.378562 G MAE: 0.247774]\n",
      "=================================================================================\n",
      "228 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.482642 G MAE: 0.706643]\n",
      "228 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.377909 G MAE: 0.247598]\n",
      "=================================================================================\n",
      "229 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.481281 G MAE: 0.706622]\n",
      "229 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.376105 G MAE: 0.247435]\n",
      "=================================================================================\n",
      "230 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.478724 G MAE: 0.706622]\n",
      "230 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.373127 G MAE: 0.247294]\n",
      "=================================================================================\n",
      "231 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.475918 G MAE: 0.706647]\n",
      "231 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.370642 G MAE: 0.247130]\n",
      "=================================================================================\n",
      "232 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.473916 G MAE: 0.706614]\n",
      "232 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.368805 G MAE: 0.246968]\n",
      "=================================================================================\n",
      "233 Train: [D loss: 0.245093, acc.: 0.00%] [G loss: 15.470931 G MAE: 0.706698]\n",
      "233 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.365628 G MAE: 0.246827]\n",
      "=================================================================================\n",
      "234 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.469248 G MAE: 0.706685]\n",
      "234 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.366483 G MAE: 0.246665]\n",
      "=================================================================================\n",
      "235 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.468886 G MAE: 0.706645]\n",
      "235 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.364806 G MAE: 0.246502]\n",
      "=================================================================================\n",
      "236 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.468228 G MAE: 0.706670]\n",
      "236 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.364747 G MAE: 0.246361]\n",
      "=================================================================================\n",
      "237 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.467724 G MAE: 0.706692]\n",
      "237 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.363387 G MAE: 0.246204]\n",
      "=================================================================================\n",
      "238 Train: [D loss: 0.245103, acc.: 0.00%] [G loss: 15.466259 G MAE: 0.706620]\n",
      "238 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.362514 G MAE: 0.246056]\n",
      "=================================================================================\n",
      "239 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.463459 G MAE: 0.706572]\n",
      "239 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.359037 G MAE: 0.245908]\n",
      "=================================================================================\n",
      "240 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.460329 G MAE: 0.706581]\n",
      "240 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.356407 G MAE: 0.245759]\n",
      "=================================================================================\n",
      "241 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.458683 G MAE: 0.706607]\n",
      "241 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.354452 G MAE: 0.245622]\n",
      "=================================================================================\n",
      "242 Train: [D loss: 0.245093, acc.: 0.00%] [G loss: 15.456696 G MAE: 0.706662]\n",
      "242 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.353813 G MAE: 0.245453]\n",
      "=================================================================================\n",
      "243 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.455149 G MAE: 0.706625]\n",
      "243 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.351021 G MAE: 0.245305]\n",
      "=================================================================================\n",
      "244 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.452566 G MAE: 0.706639]\n",
      "244 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.348876 G MAE: 0.245157]\n",
      "=================================================================================\n",
      "245 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.450603 G MAE: 0.706629]\n",
      "245 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.347181 G MAE: 0.245021]\n",
      "=================================================================================\n",
      "246 Train: [D loss: 0.245093, acc.: 0.00%] [G loss: 15.448581 G MAE: 0.706615]\n",
      "246 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.345705 G MAE: 0.244851]\n",
      "=================================================================================\n",
      "247 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.446645 G MAE: 0.706602]\n",
      "247 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.342987 G MAE: 0.244731]\n",
      "=================================================================================\n",
      "248 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.444219 G MAE: 0.706598]\n",
      "248 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.341732 G MAE: 0.244584]\n",
      "=================================================================================\n",
      "249 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.442477 G MAE: 0.706644]\n",
      "249 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.339484 G MAE: 0.244436]\n",
      "=================================================================================\n",
      "250 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.439367 G MAE: 0.706589]\n",
      "250 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.336563 G MAE: 0.244300]\n",
      "=================================================================================\n",
      "251 Train: [D loss: 0.245102, acc.: 0.00%] [G loss: 15.437259 G MAE: 0.706652]\n",
      "251 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.335120 G MAE: 0.244152]\n",
      "=================================================================================\n",
      "252 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.435213 G MAE: 0.706614]\n",
      "252 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.332854 G MAE: 0.244012]\n",
      "=================================================================================\n",
      "253 Train: [D loss: 0.245094, acc.: 0.00%] [G loss: 15.433792 G MAE: 0.706601]\n",
      "253 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.331656 G MAE: 0.243858]\n",
      "=================================================================================\n",
      "254 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.432401 G MAE: 0.706647]\n",
      "254 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.330158 G MAE: 0.243731]\n",
      "=================================================================================\n",
      "255 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.430228 G MAE: 0.706609]\n",
      "255 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.328407 G MAE: 0.243598]\n",
      "=================================================================================\n",
      "256 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.429218 G MAE: 0.706667]\n",
      "256 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.327651 G MAE: 0.243479]\n",
      "=================================================================================\n",
      "257 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.428267 G MAE: 0.706650]\n",
      "257 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.327128 G MAE: 0.243341]\n",
      "=================================================================================\n",
      "258 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.427289 G MAE: 0.706648]\n",
      "258 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.326481 G MAE: 0.243202]\n",
      "=================================================================================\n",
      "259 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.426265 G MAE: 0.706587]\n",
      "259 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.325858 G MAE: 0.243054]\n",
      "=================================================================================\n",
      "260 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.425587 G MAE: 0.706586]\n",
      "260 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.323826 G MAE: 0.242930]\n",
      "=================================================================================\n",
      "261 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.423460 G MAE: 0.706617]\n",
      "261 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.321697 G MAE: 0.242802]\n",
      "=================================================================================\n",
      "262 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.421540 G MAE: 0.706647]\n",
      "262 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.321185 G MAE: 0.242673]\n",
      "=================================================================================\n",
      "263 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.420341 G MAE: 0.706609]\n",
      "263 Test: [D loss: 0.693236, acc.: 50.00%] [G loss: 15.320243 G MAE: 0.242540]\n",
      "=================================================================================\n",
      "264 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.419982 G MAE: 0.706620]\n",
      "264 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.318565 G MAE: 0.242382]\n",
      "=================================================================================\n",
      "265 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.417937 G MAE: 0.706603]\n",
      "265 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.317734 G MAE: 0.242263]\n",
      "=================================================================================\n",
      "266 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.417902 G MAE: 0.706668]\n",
      "266 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.317121 G MAE: 0.242139]\n",
      "=================================================================================\n",
      "267 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.416054 G MAE: 0.706636]\n",
      "267 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.315863 G MAE: 0.242020]\n",
      "=================================================================================\n",
      "268 Train: [D loss: 0.245106, acc.: 0.00%] [G loss: 15.414102 G MAE: 0.706594]\n",
      "268 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.313691 G MAE: 0.241902]\n",
      "=================================================================================\n",
      "269 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.411904 G MAE: 0.706608]\n",
      "269 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.311935 G MAE: 0.241764]\n",
      "=================================================================================\n",
      "270 Train: [D loss: 0.245104, acc.: 0.00%] [G loss: 15.410861 G MAE: 0.706598]\n",
      "270 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.311008 G MAE: 0.241643]\n",
      "=================================================================================\n",
      "271 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.409720 G MAE: 0.706629]\n",
      "271 Test: [D loss: 0.693232, acc.: 50.00%] [G loss: 15.309916 G MAE: 0.241504]\n",
      "=================================================================================\n",
      "272 Train: [D loss: 0.245093, acc.: 0.00%] [G loss: 15.408016 G MAE: 0.706616]\n",
      "272 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.307299 G MAE: 0.241382]\n",
      "=================================================================================\n",
      "273 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.405912 G MAE: 0.706599]\n",
      "273 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.305390 G MAE: 0.241242]\n",
      "=================================================================================\n",
      "274 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.404252 G MAE: 0.706634]\n",
      "274 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.305662 G MAE: 0.241139]\n",
      "=================================================================================\n",
      "275 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.403870 G MAE: 0.706597]\n",
      "275 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.303711 G MAE: 0.241019]\n",
      "=================================================================================\n",
      "276 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.401369 G MAE: 0.706595]\n",
      "276 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.302512 G MAE: 0.240899]\n",
      "=================================================================================\n",
      "277 Train: [D loss: 0.245094, acc.: 0.00%] [G loss: 15.400716 G MAE: 0.706630]\n",
      "277 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.301906 G MAE: 0.240775]\n",
      "=================================================================================\n",
      "278 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.400611 G MAE: 0.706595]\n",
      "278 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.301214 G MAE: 0.240648]\n",
      "=================================================================================\n",
      "279 Train: [D loss: 0.245105, acc.: 0.00%] [G loss: 15.398773 G MAE: 0.706582]\n",
      "279 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.300061 G MAE: 0.240533]\n",
      "=================================================================================\n",
      "280 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.396520 G MAE: 0.706612]\n",
      "280 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.297114 G MAE: 0.240424]\n",
      "=================================================================================\n",
      "281 Train: [D loss: 0.245114, acc.: 0.00%] [G loss: 15.395172 G MAE: 0.706629]\n",
      "281 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.296671 G MAE: 0.240287]\n",
      "=================================================================================\n",
      "282 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.394244 G MAE: 0.706664]\n",
      "282 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.295339 G MAE: 0.240172]\n",
      "=================================================================================\n",
      "283 Train: [D loss: 0.245102, acc.: 0.00%] [G loss: 15.392059 G MAE: 0.706624]\n",
      "283 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.293474 G MAE: 0.240062]\n",
      "=================================================================================\n",
      "284 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.389765 G MAE: 0.706603]\n",
      "284 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.290525 G MAE: 0.239940]\n",
      "=================================================================================\n",
      "285 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.386990 G MAE: 0.706582]\n",
      "285 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.288496 G MAE: 0.239825]\n",
      "=================================================================================\n",
      "286 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.385511 G MAE: 0.706613]\n",
      "286 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.287029 G MAE: 0.239706]\n",
      "=================================================================================\n",
      "287 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.383796 G MAE: 0.706600]\n",
      "287 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.284831 G MAE: 0.239598]\n",
      "=================================================================================\n",
      "288 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.381239 G MAE: 0.706593]\n",
      "288 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.282861 G MAE: 0.239497]\n",
      "=================================================================================\n",
      "289 Train: [D loss: 0.245102, acc.: 0.00%] [G loss: 15.379323 G MAE: 0.706673]\n",
      "289 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.281619 G MAE: 0.239354]\n",
      "=================================================================================\n",
      "290 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.377521 G MAE: 0.706590]\n",
      "290 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.280058 G MAE: 0.239252]\n",
      "=================================================================================\n",
      "291 Train: [D loss: 0.245106, acc.: 0.00%] [G loss: 15.377784 G MAE: 0.706726]\n",
      "291 Test: [D loss: 0.693236, acc.: 50.00%] [G loss: 15.281760 G MAE: 0.239146]\n",
      "=================================================================================\n",
      "292 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.378101 G MAE: 0.706647]\n",
      "292 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.280204 G MAE: 0.239031]\n",
      "=================================================================================\n",
      "293 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.376363 G MAE: 0.706610]\n",
      "293 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.279237 G MAE: 0.238912]\n",
      "=================================================================================\n",
      "294 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.374098 G MAE: 0.706638]\n",
      "294 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.276222 G MAE: 0.238809]\n",
      "=================================================================================\n",
      "295 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.372068 G MAE: 0.706609]\n",
      "295 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.274519 G MAE: 0.238711]\n",
      "=================================================================================\n",
      "296 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.370538 G MAE: 0.706631]\n",
      "296 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.272934 G MAE: 0.238596]\n",
      "=================================================================================\n",
      "297 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.368654 G MAE: 0.706623]\n",
      "297 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.271390 G MAE: 0.238472]\n",
      "=================================================================================\n",
      "298 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.367228 G MAE: 0.706616]\n",
      "298 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.269357 G MAE: 0.238358]\n",
      "=================================================================================\n",
      "299 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.364900 G MAE: 0.706605]\n",
      "299 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.267782 G MAE: 0.238268]\n",
      "=================================================================================\n",
      "300 Train: [D loss: 0.245102, acc.: 0.00%] [G loss: 15.362563 G MAE: 0.706577]\n",
      "300 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.265761 G MAE: 0.238148]\n",
      "=================================================================================\n",
      "301 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.361001 G MAE: 0.706580]\n",
      "301 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.264456 G MAE: 0.238048]\n",
      "=================================================================================\n",
      "302 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.359422 G MAE: 0.706603]\n",
      "302 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.262404 G MAE: 0.237951]\n",
      "=================================================================================\n",
      "303 Train: [D loss: 0.245103, acc.: 0.00%] [G loss: 15.358118 G MAE: 0.706631]\n",
      "303 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.261172 G MAE: 0.237850]\n",
      "=================================================================================\n",
      "304 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.356905 G MAE: 0.706615]\n",
      "304 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.260657 G MAE: 0.237738]\n",
      "=================================================================================\n",
      "305 Train: [D loss: 0.245094, acc.: 0.00%] [G loss: 15.356537 G MAE: 0.706626]\n",
      "305 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.260072 G MAE: 0.237617]\n",
      "=================================================================================\n",
      "306 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.354845 G MAE: 0.706559]\n",
      "306 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.259183 G MAE: 0.237516]\n",
      "=================================================================================\n",
      "307 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.353982 G MAE: 0.706638]\n",
      "307 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.258208 G MAE: 0.237434]\n",
      "=================================================================================\n",
      "308 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.352410 G MAE: 0.706594]\n",
      "308 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.255915 G MAE: 0.237326]\n",
      "=================================================================================\n",
      "309 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.351003 G MAE: 0.706610]\n",
      "309 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.255345 G MAE: 0.237219]\n",
      "=================================================================================\n",
      "310 Train: [D loss: 0.245093, acc.: 0.00%] [G loss: 15.350569 G MAE: 0.706617]\n",
      "310 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.253848 G MAE: 0.237109]\n",
      "=================================================================================\n",
      "311 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.348059 G MAE: 0.706584]\n",
      "311 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.251599 G MAE: 0.237009]\n",
      "=================================================================================\n",
      "312 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.346832 G MAE: 0.706591]\n",
      "312 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.251326 G MAE: 0.236895]\n",
      "=================================================================================\n",
      "313 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.344963 G MAE: 0.706640]\n",
      "313 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.248916 G MAE: 0.236800]\n",
      "=================================================================================\n",
      "314 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.343619 G MAE: 0.706585]\n",
      "314 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.248221 G MAE: 0.236722]\n",
      "=================================================================================\n",
      "315 Train: [D loss: 0.245094, acc.: 0.00%] [G loss: 15.341879 G MAE: 0.706643]\n",
      "315 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.245200 G MAE: 0.236605]\n",
      "=================================================================================\n",
      "316 Train: [D loss: 0.245094, acc.: 0.00%] [G loss: 15.338813 G MAE: 0.706654]\n",
      "316 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.243389 G MAE: 0.236509]\n",
      "=================================================================================\n",
      "317 Train: [D loss: 0.245094, acc.: 0.00%] [G loss: 15.337078 G MAE: 0.706589]\n",
      "317 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.242475 G MAE: 0.236400]\n",
      "=================================================================================\n",
      "318 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.336281 G MAE: 0.706573]\n",
      "318 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.241402 G MAE: 0.236306]\n",
      "=================================================================================\n",
      "319 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.335738 G MAE: 0.706616]\n",
      "319 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.241044 G MAE: 0.236210]\n",
      "=================================================================================\n",
      "320 Train: [D loss: 0.245104, acc.: 0.00%] [G loss: 15.334387 G MAE: 0.706586]\n",
      "320 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.239708 G MAE: 0.236112]\n",
      "=================================================================================\n",
      "321 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.332536 G MAE: 0.706625]\n",
      "321 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.237589 G MAE: 0.236023]\n",
      "=================================================================================\n",
      "322 Train: [D loss: 0.245108, acc.: 0.00%] [G loss: 15.330724 G MAE: 0.706568]\n",
      "322 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.236070 G MAE: 0.235926]\n",
      "=================================================================================\n",
      "323 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.329108 G MAE: 0.706637]\n",
      "323 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.235000 G MAE: 0.235836]\n",
      "=================================================================================\n",
      "324 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.328040 G MAE: 0.706598]\n",
      "324 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.233426 G MAE: 0.235743]\n",
      "=================================================================================\n",
      "325 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.326006 G MAE: 0.706625]\n",
      "325 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.231604 G MAE: 0.235633]\n",
      "=================================================================================\n",
      "326 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.325115 G MAE: 0.706664]\n",
      "326 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.231143 G MAE: 0.235553]\n",
      "=================================================================================\n",
      "327 Train: [D loss: 0.245093, acc.: 0.00%] [G loss: 15.324461 G MAE: 0.706599]\n",
      "327 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.229659 G MAE: 0.235458]\n",
      "=================================================================================\n",
      "328 Train: [D loss: 0.245109, acc.: 0.00%] [G loss: 15.322550 G MAE: 0.706615]\n",
      "328 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.228334 G MAE: 0.235367]\n",
      "=================================================================================\n",
      "329 Train: [D loss: 0.245094, acc.: 0.00%] [G loss: 15.320716 G MAE: 0.706590]\n",
      "329 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.226019 G MAE: 0.235279]\n",
      "=================================================================================\n",
      "330 Train: [D loss: 0.245107, acc.: 0.00%] [G loss: 15.318858 G MAE: 0.706590]\n",
      "330 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.225693 G MAE: 0.235187]\n",
      "=================================================================================\n",
      "331 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.317527 G MAE: 0.706622]\n",
      "331 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.223274 G MAE: 0.235093]\n",
      "=================================================================================\n",
      "332 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.315139 G MAE: 0.706599]\n",
      "332 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.221067 G MAE: 0.234992]\n",
      "=================================================================================\n",
      "333 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.313402 G MAE: 0.706604]\n",
      "333 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.220026 G MAE: 0.234907]\n",
      "=================================================================================\n",
      "334 Train: [D loss: 0.245093, acc.: 0.00%] [G loss: 15.313144 G MAE: 0.706638]\n",
      "334 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.219045 G MAE: 0.234805]\n",
      "=================================================================================\n",
      "335 Train: [D loss: 0.245094, acc.: 0.00%] [G loss: 15.310913 G MAE: 0.706615]\n",
      "335 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.217058 G MAE: 0.234718]\n",
      "=================================================================================\n",
      "336 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.309361 G MAE: 0.706615]\n",
      "336 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.216401 G MAE: 0.234634]\n",
      "=================================================================================\n",
      "337 Train: [D loss: 0.245094, acc.: 0.00%] [G loss: 15.308128 G MAE: 0.706628]\n",
      "337 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.215202 G MAE: 0.234531]\n",
      "=================================================================================\n",
      "338 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.307820 G MAE: 0.706657]\n",
      "338 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.214958 G MAE: 0.234456]\n",
      "=================================================================================\n",
      "339 Train: [D loss: 0.245102, acc.: 0.00%] [G loss: 15.306721 G MAE: 0.706631]\n",
      "339 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.213643 G MAE: 0.234364]\n",
      "=================================================================================\n",
      "340 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.304436 G MAE: 0.706624]\n",
      "340 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.211335 G MAE: 0.234288]\n",
      "=================================================================================\n",
      "341 Train: [D loss: 0.245094, acc.: 0.00%] [G loss: 15.303293 G MAE: 0.706617]\n",
      "341 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.209297 G MAE: 0.234178]\n",
      "=================================================================================\n",
      "342 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.300846 G MAE: 0.706642]\n",
      "342 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.206438 G MAE: 0.234093]\n",
      "=================================================================================\n",
      "343 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.298417 G MAE: 0.706598]\n",
      "343 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.205155 G MAE: 0.234021]\n",
      "=================================================================================\n",
      "344 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.297254 G MAE: 0.706553]\n",
      "344 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.204470 G MAE: 0.233934]\n",
      "=================================================================================\n",
      "347 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.292047 G MAE: 0.706597]\n",
      "347 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.199842 G MAE: 0.233663]\n",
      "=================================================================================\n",
      "348 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.290931 G MAE: 0.706599]\n",
      "348 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.197347 G MAE: 0.233579]\n",
      "=================================================================================\n",
      "349 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.288323 G MAE: 0.706573]\n",
      "349 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.195879 G MAE: 0.233500]\n",
      "=================================================================================\n",
      "350 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.286102 G MAE: 0.706600]\n",
      "350 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.193824 G MAE: 0.233416]\n",
      "=================================================================================\n",
      "351 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.284625 G MAE: 0.706587]\n",
      "351 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.191812 G MAE: 0.233329]\n",
      "=================================================================================\n",
      "352 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.282647 G MAE: 0.706568]\n",
      "352 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.191248 G MAE: 0.233249]\n",
      "=================================================================================\n",
      "353 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.281731 G MAE: 0.706601]\n",
      "353 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.189404 G MAE: 0.233162]\n",
      "=================================================================================\n",
      "354 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.280253 G MAE: 0.706595]\n",
      "354 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.188230 G MAE: 0.233078]\n",
      "=================================================================================\n",
      "355 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.278816 G MAE: 0.706626]\n",
      "355 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.186589 G MAE: 0.232997]\n",
      "=================================================================================\n",
      "356 Train: [D loss: 0.245102, acc.: 0.00%] [G loss: 15.277173 G MAE: 0.706592]\n",
      "356 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.184661 G MAE: 0.232919]\n",
      "=================================================================================\n",
      "357 Train: [D loss: 0.245106, acc.: 0.00%] [G loss: 15.274816 G MAE: 0.706563]\n",
      "357 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.182768 G MAE: 0.232827]\n",
      "=================================================================================\n",
      "358 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.273047 G MAE: 0.706641]\n",
      "358 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.181235 G MAE: 0.232749]\n",
      "=================================================================================\n",
      "359 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.271567 G MAE: 0.706610]\n",
      "359 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.179930 G MAE: 0.232682]\n",
      "=================================================================================\n",
      "360 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.271196 G MAE: 0.706653]\n",
      "360 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.180760 G MAE: 0.232594]\n",
      "=================================================================================\n",
      "361 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.270584 G MAE: 0.706612]\n",
      "361 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.178918 G MAE: 0.232521]\n",
      "=================================================================================\n",
      "362 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.269457 G MAE: 0.706567]\n",
      "362 Test: [D loss: 0.693232, acc.: 50.00%] [G loss: 15.177931 G MAE: 0.232444]\n",
      "=================================================================================\n",
      "363 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.268165 G MAE: 0.706603]\n",
      "363 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.176824 G MAE: 0.232348]\n",
      "=================================================================================\n",
      "364 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.266980 G MAE: 0.706620]\n",
      "364 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.175739 G MAE: 0.232261]\n",
      "=================================================================================\n",
      "365 Train: [D loss: 0.245094, acc.: 0.00%] [G loss: 15.265684 G MAE: 0.706629]\n",
      "365 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.174354 G MAE: 0.232206]\n",
      "=================================================================================\n",
      "366 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.264445 G MAE: 0.706573]\n",
      "366 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.172661 G MAE: 0.232115]\n",
      "=================================================================================\n",
      "367 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.262865 G MAE: 0.706615]\n",
      "367 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.171994 G MAE: 0.232036]\n",
      "=================================================================================\n",
      "368 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.261396 G MAE: 0.706643]\n",
      "368 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.170483 G MAE: 0.231967]\n",
      "=================================================================================\n",
      "369 Train: [D loss: 0.245094, acc.: 0.00%] [G loss: 15.259676 G MAE: 0.706607]\n",
      "369 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.169006 G MAE: 0.231887]\n",
      "=================================================================================\n",
      "370 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.259677 G MAE: 0.706596]\n",
      "370 Test: [D loss: 0.693232, acc.: 50.00%] [G loss: 15.168825 G MAE: 0.231812]\n",
      "=================================================================================\n",
      "371 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.258214 G MAE: 0.706553]\n",
      "371 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.168040 G MAE: 0.231750]\n",
      "=================================================================================\n",
      "372 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.255894 G MAE: 0.706571]\n",
      "372 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.165511 G MAE: 0.231667]\n",
      "=================================================================================\n",
      "373 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.254378 G MAE: 0.706604]\n",
      "373 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.163628 G MAE: 0.231584]\n",
      "=================================================================================\n",
      "374 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.253797 G MAE: 0.706620]\n",
      "374 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.163125 G MAE: 0.231513]\n",
      "=================================================================================\n",
      "375 Train: [D loss: 0.245093, acc.: 0.00%] [G loss: 15.252719 G MAE: 0.706545]\n",
      "375 Test: [D loss: 0.693232, acc.: 50.00%] [G loss: 15.161994 G MAE: 0.231431]\n",
      "=================================================================================\n",
      "376 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.251362 G MAE: 0.706595]\n",
      "376 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.161021 G MAE: 0.231366]\n",
      "=================================================================================\n",
      "377 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.251159 G MAE: 0.706626]\n",
      "377 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.161873 G MAE: 0.231288]\n",
      "=================================================================================\n",
      "378 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.252077 G MAE: 0.706642]\n",
      "378 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.163486 G MAE: 0.231210]\n",
      "=================================================================================\n",
      "379 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.251578 G MAE: 0.706596]\n",
      "379 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.161064 G MAE: 0.231127]\n",
      "=================================================================================\n",
      "380 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.249252 G MAE: 0.706611]\n",
      "380 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.159577 G MAE: 0.231056]\n",
      "=================================================================================\n",
      "381 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.247707 G MAE: 0.706611]\n",
      "381 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.158307 G MAE: 0.230991]\n",
      "=================================================================================\n",
      "382 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.246971 G MAE: 0.706578]\n",
      "382 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.157352 G MAE: 0.230921]\n",
      "=================================================================================\n",
      "383 Train: [D loss: 0.245105, acc.: 0.00%] [G loss: 15.246147 G MAE: 0.706644]\n",
      "383 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.156354 G MAE: 0.230847]\n",
      "=================================================================================\n",
      "384 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.244492 G MAE: 0.706610]\n",
      "384 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.154866 G MAE: 0.230779]\n",
      "=================================================================================\n",
      "385 Train: [D loss: 0.245112, acc.: 0.00%] [G loss: 15.244262 G MAE: 0.706613]\n",
      "385 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.154609 G MAE: 0.230698]\n",
      "=================================================================================\n",
      "386 Train: [D loss: 0.245093, acc.: 0.00%] [G loss: 15.242224 G MAE: 0.706590]\n",
      "386 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.152324 G MAE: 0.230624]\n",
      "=================================================================================\n",
      "387 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.240870 G MAE: 0.706613]\n",
      "387 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.151264 G MAE: 0.230574]\n",
      "=================================================================================\n",
      "388 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.238691 G MAE: 0.706580]\n",
      "388 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.148402 G MAE: 0.230489]\n",
      "=================================================================================\n",
      "389 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.235434 G MAE: 0.706595]\n",
      "389 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.146056 G MAE: 0.230400]\n",
      "=================================================================================\n",
      "390 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.234735 G MAE: 0.706642]\n",
      "390 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.145099 G MAE: 0.230335]\n",
      "=================================================================================\n",
      "391 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.233076 G MAE: 0.706572]\n",
      "391 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.143235 G MAE: 0.230271]\n",
      "=================================================================================\n",
      "392 Train: [D loss: 0.245103, acc.: 0.00%] [G loss: 15.231070 G MAE: 0.706562]\n",
      "392 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.141345 G MAE: 0.230210]\n",
      "=================================================================================\n",
      "393 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.229043 G MAE: 0.706646]\n",
      "393 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.140241 G MAE: 0.230135]\n",
      "=================================================================================\n",
      "394 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.227810 G MAE: 0.706582]\n",
      "394 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.138432 G MAE: 0.230053]\n",
      "=================================================================================\n",
      "395 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.226528 G MAE: 0.706615]\n",
      "395 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.137508 G MAE: 0.229999]\n",
      "=================================================================================\n",
      "396 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.225738 G MAE: 0.706630]\n",
      "396 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.136603 G MAE: 0.229940]\n",
      "=================================================================================\n",
      "397 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.224958 G MAE: 0.706605]\n",
      "397 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.135756 G MAE: 0.229872]\n",
      "=================================================================================\n",
      "398 Train: [D loss: 0.245091, acc.: 0.00%] [G loss: 15.223308 G MAE: 0.706571]\n",
      "398 Test: [D loss: 0.693232, acc.: 50.00%] [G loss: 15.134234 G MAE: 0.229803]\n",
      "=================================================================================\n",
      "399 Train: [D loss: 0.245102, acc.: 0.00%] [G loss: 15.221698 G MAE: 0.706562]\n",
      "399 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.133338 G MAE: 0.229734]\n",
      "=================================================================================\n",
      "400 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.220164 G MAE: 0.706582]\n",
      "400 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.131616 G MAE: 0.229657]\n",
      "=================================================================================\n",
      "401 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.219850 G MAE: 0.706579]\n",
      "401 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.131502 G MAE: 0.229592]\n",
      "=================================================================================\n",
      "402 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.219789 G MAE: 0.706626]\n",
      "402 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.130906 G MAE: 0.229526]\n",
      "=================================================================================\n",
      "403 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.217754 G MAE: 0.706619]\n",
      "403 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.128838 G MAE: 0.229473]\n",
      "=================================================================================\n",
      "404 Train: [D loss: 0.245093, acc.: 0.00%] [G loss: 15.216295 G MAE: 0.706589]\n",
      "404 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.127449 G MAE: 0.229384]\n",
      "=================================================================================\n",
      "405 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.215759 G MAE: 0.706617]\n",
      "405 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.127429 G MAE: 0.229319]\n",
      "=================================================================================\n",
      "406 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.214012 G MAE: 0.706596]\n",
      "406 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.125612 G MAE: 0.229256]\n",
      "=================================================================================\n",
      "407 Train: [D loss: 0.245102, acc.: 0.00%] [G loss: 15.212865 G MAE: 0.706629]\n",
      "407 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.125176 G MAE: 0.229186]\n",
      "=================================================================================\n",
      "408 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.212290 G MAE: 0.706615]\n",
      "408 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.123780 G MAE: 0.229119]\n",
      "=================================================================================\n",
      "409 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.211102 G MAE: 0.706593]\n",
      "409 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.122925 G MAE: 0.229064]\n",
      "=================================================================================\n",
      "410 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.210310 G MAE: 0.706575]\n",
      "410 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.122785 G MAE: 0.229007]\n",
      "=================================================================================\n",
      "411 Train: [D loss: 0.245115, acc.: 0.00%] [G loss: 15.209077 G MAE: 0.706603]\n",
      "411 Test: [D loss: 0.693236, acc.: 50.00%] [G loss: 15.120692 G MAE: 0.228939]\n",
      "=================================================================================\n",
      "412 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.207368 G MAE: 0.706643]\n",
      "412 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.119648 G MAE: 0.228873]\n",
      "=================================================================================\n",
      "413 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.207028 G MAE: 0.706586]\n",
      "413 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.119123 G MAE: 0.228810]\n",
      "=================================================================================\n",
      "414 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.205191 G MAE: 0.706632]\n",
      "414 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.116894 G MAE: 0.228747]\n",
      "=================================================================================\n",
      "415 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.203402 G MAE: 0.706620]\n",
      "415 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.116864 G MAE: 0.228663]\n",
      "=================================================================================\n",
      "416 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.203479 G MAE: 0.706615]\n",
      "416 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.114912 G MAE: 0.228620]\n",
      "=================================================================================\n",
      "417 Train: [D loss: 0.245103, acc.: 0.00%] [G loss: 15.201182 G MAE: 0.706564]\n",
      "417 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.113406 G MAE: 0.228553]\n",
      "=================================================================================\n",
      "418 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.199696 G MAE: 0.706590]\n",
      "418 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.111232 G MAE: 0.228500]\n",
      "=================================================================================\n",
      "419 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.198187 G MAE: 0.706590]\n",
      "419 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.111008 G MAE: 0.228429]\n",
      "=================================================================================\n",
      "420 Train: [D loss: 0.245104, acc.: 0.00%] [G loss: 15.196713 G MAE: 0.706590]\n",
      "420 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.109415 G MAE: 0.228368]\n",
      "=================================================================================\n",
      "421 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.195496 G MAE: 0.706608]\n",
      "421 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.108154 G MAE: 0.228299]\n",
      "=================================================================================\n",
      "422 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.194671 G MAE: 0.706610]\n",
      "422 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.107131 G MAE: 0.228246]\n",
      "=================================================================================\n",
      "423 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.193113 G MAE: 0.706589]\n",
      "423 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.105927 G MAE: 0.228167]\n",
      "=================================================================================\n",
      "424 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.191692 G MAE: 0.706603]\n",
      "424 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.104290 G MAE: 0.228128]\n",
      "=================================================================================\n",
      "425 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.190343 G MAE: 0.706612]\n",
      "425 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.103367 G MAE: 0.228071]\n",
      "=================================================================================\n",
      "426 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.189194 G MAE: 0.706616]\n",
      "426 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.102168 G MAE: 0.227992]\n",
      "=================================================================================\n",
      "427 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.187893 G MAE: 0.706649]\n",
      "427 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.101074 G MAE: 0.227937]\n",
      "=================================================================================\n",
      "428 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.186925 G MAE: 0.706567]\n",
      "428 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.100360 G MAE: 0.227873]\n",
      "=================================================================================\n",
      "429 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.186708 G MAE: 0.706602]\n",
      "429 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.101087 G MAE: 0.227826]\n",
      "=================================================================================\n",
      "430 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.185640 G MAE: 0.706609]\n",
      "430 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.098747 G MAE: 0.227764]\n",
      "=================================================================================\n",
      "431 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.183992 G MAE: 0.706562]\n",
      "431 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.096855 G MAE: 0.227706]\n",
      "=================================================================================\n",
      "432 Train: [D loss: 0.245104, acc.: 0.00%] [G loss: 15.181870 G MAE: 0.706558]\n",
      "432 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.095587 G MAE: 0.227639]\n",
      "=================================================================================\n",
      "433 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.180778 G MAE: 0.706604]\n",
      "433 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.094422 G MAE: 0.227597]\n",
      "=================================================================================\n",
      "434 Train: [D loss: 0.245104, acc.: 0.00%] [G loss: 15.179452 G MAE: 0.706600]\n",
      "434 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.093486 G MAE: 0.227537]\n",
      "=================================================================================\n",
      "435 Train: [D loss: 0.245094, acc.: 0.00%] [G loss: 15.178547 G MAE: 0.706600]\n",
      "435 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.091807 G MAE: 0.227450]\n",
      "=================================================================================\n",
      "436 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.176815 G MAE: 0.706568]\n",
      "436 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.090475 G MAE: 0.227393]\n",
      "=================================================================================\n",
      "437 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.175733 G MAE: 0.706577]\n",
      "437 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.090240 G MAE: 0.227348]\n",
      "=================================================================================\n",
      "438 Train: [D loss: 0.245103, acc.: 0.00%] [G loss: 15.175392 G MAE: 0.706632]\n",
      "438 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.090530 G MAE: 0.227286]\n",
      "=================================================================================\n",
      "439 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.175447 G MAE: 0.706603]\n",
      "439 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.089455 G MAE: 0.227246]\n",
      "=================================================================================\n",
      "440 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.173945 G MAE: 0.706616]\n",
      "440 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.087077 G MAE: 0.227168]\n",
      "=================================================================================\n",
      "441 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.171305 G MAE: 0.706616]\n",
      "441 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.085609 G MAE: 0.227132]\n",
      "=================================================================================\n",
      "442 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.170711 G MAE: 0.706601]\n",
      "442 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.084381 G MAE: 0.227073]\n",
      "=================================================================================\n",
      "443 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.168514 G MAE: 0.706574]\n",
      "443 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.082613 G MAE: 0.227015]\n",
      "=================================================================================\n",
      "444 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.167519 G MAE: 0.706545]\n",
      "444 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.082415 G MAE: 0.226962]\n",
      "=================================================================================\n",
      "445 Train: [D loss: 0.245102, acc.: 0.00%] [G loss: 15.165941 G MAE: 0.706594]\n",
      "445 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.079640 G MAE: 0.226897]\n",
      "=================================================================================\n",
      "446 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.164190 G MAE: 0.706553]\n",
      "446 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.078539 G MAE: 0.226841]\n",
      "=================================================================================\n",
      "447 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.162957 G MAE: 0.706578]\n",
      "447 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.076923 G MAE: 0.226774]\n",
      "=================================================================================\n",
      "448 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.161007 G MAE: 0.706589]\n",
      "448 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.074013 G MAE: 0.226731]\n",
      "=================================================================================\n",
      "449 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.159013 G MAE: 0.706581]\n",
      "449 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.074002 G MAE: 0.226674]\n",
      "=================================================================================\n",
      "450 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.158678 G MAE: 0.706620]\n",
      "450 Test: [D loss: 0.693232, acc.: 50.00%] [G loss: 15.072746 G MAE: 0.226615]\n",
      "=================================================================================\n",
      "451 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.156187 G MAE: 0.706554]\n",
      "451 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.070251 G MAE: 0.226561]\n",
      "=================================================================================\n",
      "452 Train: [D loss: 0.245094, acc.: 0.00%] [G loss: 15.154500 G MAE: 0.706595]\n",
      "452 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.067967 G MAE: 0.226510]\n",
      "=================================================================================\n",
      "453 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.153362 G MAE: 0.706563]\n",
      "453 Test: [D loss: 0.693232, acc.: 50.00%] [G loss: 15.068720 G MAE: 0.226443]\n",
      "=================================================================================\n",
      "454 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.152571 G MAE: 0.706616]\n",
      "454 Test: [D loss: 0.693236, acc.: 50.00%] [G loss: 15.066566 G MAE: 0.226397]\n",
      "=================================================================================\n",
      "455 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.150468 G MAE: 0.706605]\n",
      "455 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.065047 G MAE: 0.226341]\n",
      "=================================================================================\n",
      "456 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.149075 G MAE: 0.706595]\n",
      "456 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.064961 G MAE: 0.226290]\n",
      "=================================================================================\n",
      "457 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.148567 G MAE: 0.706585]\n",
      "457 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.063683 G MAE: 0.226232]\n",
      "=================================================================================\n",
      "458 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.147559 G MAE: 0.706581]\n",
      "458 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.063182 G MAE: 0.226188]\n",
      "=================================================================================\n",
      "459 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.148143 G MAE: 0.706651]\n",
      "459 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.062662 G MAE: 0.226121]\n",
      "=================================================================================\n",
      "460 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.145822 G MAE: 0.706586]\n",
      "460 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.060270 G MAE: 0.226070]\n",
      "=================================================================================\n",
      "461 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.144686 G MAE: 0.706612]\n",
      "461 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.060656 G MAE: 0.226016]\n",
      "=================================================================================\n",
      "462 Train: [D loss: 0.245104, acc.: 0.00%] [G loss: 15.144481 G MAE: 0.706641]\n",
      "462 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.060265 G MAE: 0.225963]\n",
      "=================================================================================\n",
      "463 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.143325 G MAE: 0.706618]\n",
      "463 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.059166 G MAE: 0.225909]\n",
      "=================================================================================\n",
      "464 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.142559 G MAE: 0.706606]\n",
      "464 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.058450 G MAE: 0.225882]\n",
      "=================================================================================\n",
      "465 Train: [D loss: 0.245093, acc.: 0.00%] [G loss: 15.141555 G MAE: 0.706605]\n",
      "465 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.056940 G MAE: 0.225818]\n",
      "=================================================================================\n",
      "466 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.140817 G MAE: 0.706643]\n",
      "466 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.056546 G MAE: 0.225758]\n",
      "=================================================================================\n",
      "467 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.140999 G MAE: 0.706614]\n",
      "467 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.057342 G MAE: 0.225700]\n",
      "=================================================================================\n",
      "468 Train: [D loss: 0.245094, acc.: 0.00%] [G loss: 15.141814 G MAE: 0.706600]\n",
      "468 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.058166 G MAE: 0.225668]\n",
      "=================================================================================\n",
      "469 Train: [D loss: 0.245105, acc.: 0.00%] [G loss: 15.141735 G MAE: 0.706605]\n",
      "469 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.057742 G MAE: 0.225616]\n",
      "=================================================================================\n",
      "470 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.140783 G MAE: 0.706602]\n",
      "470 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.055534 G MAE: 0.225542]\n",
      "=================================================================================\n",
      "471 Train: [D loss: 0.245094, acc.: 0.00%] [G loss: 15.139492 G MAE: 0.706620]\n",
      "471 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.055517 G MAE: 0.225501]\n",
      "=================================================================================\n",
      "472 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.138466 G MAE: 0.706593]\n",
      "472 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.054287 G MAE: 0.225471]\n",
      "=================================================================================\n",
      "473 Train: [D loss: 0.245103, acc.: 0.00%] [G loss: 15.138218 G MAE: 0.706598]\n",
      "473 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.054708 G MAE: 0.225398]\n",
      "=================================================================================\n",
      "474 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.137962 G MAE: 0.706626]\n",
      "474 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.053813 G MAE: 0.225356]\n",
      "=================================================================================\n",
      "475 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.136710 G MAE: 0.706596]\n",
      "475 Test: [D loss: 0.693235, acc.: 50.00%] [G loss: 15.053185 G MAE: 0.225309]\n",
      "=================================================================================\n",
      "476 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.136169 G MAE: 0.706620]\n",
      "476 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.052270 G MAE: 0.225254]\n",
      "=================================================================================\n",
      "477 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.136065 G MAE: 0.706650]\n",
      "477 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.052453 G MAE: 0.225195]\n",
      "=================================================================================\n",
      "478 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.135698 G MAE: 0.706610]\n",
      "478 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.052036 G MAE: 0.225146]\n",
      "=================================================================================\n",
      "479 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.135006 G MAE: 0.706595]\n",
      "479 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.051409 G MAE: 0.225103]\n",
      "=================================================================================\n",
      "480 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.133485 G MAE: 0.706612]\n",
      "480 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.049905 G MAE: 0.225050]\n",
      "=================================================================================\n",
      "481 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.132407 G MAE: 0.706622]\n",
      "481 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.049074 G MAE: 0.224991]\n",
      "=================================================================================\n",
      "482 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.131398 G MAE: 0.706530]\n",
      "482 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.048005 G MAE: 0.224965]\n",
      "=================================================================================\n",
      "483 Train: [D loss: 0.245097, acc.: 0.00%] [G loss: 15.130381 G MAE: 0.706606]\n",
      "483 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.046117 G MAE: 0.224915]\n",
      "=================================================================================\n",
      "484 Train: [D loss: 0.245095, acc.: 0.00%] [G loss: 15.128733 G MAE: 0.706582]\n",
      "484 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.044895 G MAE: 0.224853]\n",
      "=================================================================================\n",
      "485 Train: [D loss: 0.245094, acc.: 0.00%] [G loss: 15.127148 G MAE: 0.706603]\n",
      "485 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.043601 G MAE: 0.224822]\n",
      "=================================================================================\n",
      "486 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.125658 G MAE: 0.706568]\n",
      "486 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.041436 G MAE: 0.224761]\n",
      "=================================================================================\n",
      "487 Train: [D loss: 0.245100, acc.: 0.00%] [G loss: 15.123912 G MAE: 0.706589]\n",
      "487 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.040748 G MAE: 0.224722]\n",
      "=================================================================================\n",
      "488 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.123310 G MAE: 0.706593]\n",
      "488 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.039996 G MAE: 0.224664]\n",
      "=================================================================================\n",
      "489 Train: [D loss: 0.245094, acc.: 0.00%] [G loss: 15.121964 G MAE: 0.706588]\n",
      "489 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.038277 G MAE: 0.224604]\n",
      "=================================================================================\n",
      "490 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.120929 G MAE: 0.706588]\n",
      "490 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.037767 G MAE: 0.224570]\n",
      "=================================================================================\n",
      "491 Train: [D loss: 0.245101, acc.: 0.00%] [G loss: 15.120136 G MAE: 0.706590]\n",
      "491 Test: [D loss: 0.693236, acc.: 50.00%] [G loss: 15.037977 G MAE: 0.224531]\n",
      "=================================================================================\n",
      "492 Train: [D loss: 0.245102, acc.: 0.00%] [G loss: 15.120369 G MAE: 0.706628]\n",
      "492 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.038395 G MAE: 0.224473]\n",
      "=================================================================================\n",
      "493 Train: [D loss: 0.245099, acc.: 0.00%] [G loss: 15.120381 G MAE: 0.706596]\n",
      "493 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.037609 G MAE: 0.224445]\n",
      "=================================================================================\n",
      "494 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.119749 G MAE: 0.706619]\n",
      "494 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.036854 G MAE: 0.224398]\n",
      "=================================================================================\n",
      "495 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.119515 G MAE: 0.706600]\n",
      "495 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.036486 G MAE: 0.224333]\n",
      "=================================================================================\n",
      "496 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.119747 G MAE: 0.706593]\n",
      "496 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.037538 G MAE: 0.224279]\n",
      "=================================================================================\n",
      "497 Train: [D loss: 0.245098, acc.: 0.00%] [G loss: 15.119526 G MAE: 0.706590]\n",
      "497 Test: [D loss: 0.693234, acc.: 50.00%] [G loss: 15.036835 G MAE: 0.224250]\n",
      "=================================================================================\n",
      "498 Train: [D loss: 0.245121, acc.: 0.00%] [G loss: 15.119099 G MAE: 0.706602]\n",
      "498 Test: [D loss: 0.693236, acc.: 50.00%] [G loss: 15.037756 G MAE: 0.224204]\n",
      "=================================================================================\n",
      "499 Train: [D loss: 0.245096, acc.: 0.00%] [G loss: 15.119478 G MAE: 0.706634]\n",
      "499 Test: [D loss: 0.693233, acc.: 50.00%] [G loss: 15.036556 G MAE: 0.224171]\n",
      "=================================================================================\n"
     ]
    }
   ],
   "source": [
    "valid = np.ones((batch_size, 1))\n",
    "fake = np.zeros((batch_size, 1))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    n = 0\n",
    "    g_total_loss = 0\n",
    "    g_total_mae = 0\n",
    "    d_total_loss = 0\n",
    "    d_total_accuracy = 0\n",
    "    \n",
    "    for X, y in data_generator:\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        \n",
    "        \n",
    "        # Train the discriminator and calculate loss and accuracy\n",
    "        y_fake = generator.predict(X)\n",
    "        \n",
    "        # then use discriminator train_on_batch twice.\n",
    "        # once with y as input and once with the output of generator.predict as input.\n",
    "        d_loss_real = discriminator.train_on_batch(y, valid)\n",
    "        d_loss_fake = discriminator.train_on_batch(y_fake, fake)\n",
    "        \n",
    "        d_loss = d_loss_real[0] + d_loss_fake[0]\n",
    "        d_accuracy = d_loss_real[1] + d_loss_fake[1]\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "\n",
    "        # Train the generator and calculate loss and MAE\n",
    "        g_losses = gan_model.train_on_batch(X, [y, valid])\n",
    "        \n",
    "        g_loss = g_losses[0]\n",
    "        g_mae = g_losses[2]\n",
    "        \n",
    "        d_total_loss = (d_loss + d_total_loss) / 2\n",
    "        d_total_accuracy = (d_accuracy + d_total_accuracy) / 2\n",
    "        g_total_loss = g_loss + g_total_loss\n",
    "        g_total_mae = g_mae + g_total_mae\n",
    "        n += 1\n",
    "        \n",
    "#         print(\"%d Train: [D loss: %f, acc.: %.2f%%] [G loss: %f G MAE: %f]\" % \n",
    "#               (epoch, d_total_loss / n, d_total_accuracy / n, g_total_loss / n, g_total_mae / n))\n",
    "        \n",
    "    y_test_fake = generator.predict(x_test)\n",
    "    test_score_real = gan_model.evaluate(x_test, [y_test, np.ones((y_test.shape[0], 1))], verbose=0)\n",
    "    test_score_fake = gan_model.evaluate(x_test, [y_test_fake, np.zeros((y_test.shape[0], 1))], verbose=0)\n",
    "    test_score = np.add(test_score_real, test_score_fake) / 2\n",
    "    \n",
    "    test_d_loss = test_score[2]\n",
    "    test_d_accuracy = test_score[3]\n",
    "    test_g_loss = test_score[0]\n",
    "    test_g_mae = test_score_real[1]\n",
    "    \n",
    "    \n",
    "    print (\"%d Train: [D loss: %f, acc.: %.2f%%] [G loss: %f G MAE: %f]\" % \n",
    "           (epoch, d_total_loss / n, d_total_accuracy * 100 / n, g_total_loss / n, g_total_mae / n))\n",
    "    print (\"%d Test: [D loss: %f, acc.: %.2f%%] [G loss: %f G MAE: %f]\" % \n",
    "           (epoch, test_d_loss, test_d_accuracy * 100, test_g_loss, test_g_mae))\n",
    "    print('=================================================================================')\n",
    "    \n",
    "    data_generator.on_epoch_end()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "EM84jU_vtPOp",
    "outputId": "0eaa1712-9432-4097-93a0-47771397e916"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "731/731 [==============================] - 0s 69us/step\n",
      "2190/2190 [==============================] - 0s 68us/step\n",
      "Train MAE: 0.224171\n",
      "Test MAE: 0.180115\n"
     ]
    }
   ],
   "source": [
    "# report MAE on train and test data\n",
    "\n",
    "test_score = gan_model.evaluate(x_test, [y_test, np.ones((y_test.shape[0], 1))])\n",
    "train_score = gan_model.evaluate(x_train, [y_train, np.ones((y_train.shape[0], 1))])\n",
    "\n",
    "print (\"Train MAE: %f\" % test_score[1])\n",
    "print (\"Test MAE: %f\" % train_score[1])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW6_ML.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "bioml-hw6",
   "language": "python",
   "name": "bioml-hw6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
